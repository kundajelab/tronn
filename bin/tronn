#!/usr/bin/env python

"""Description: TRONN main executable
"""

import os
import sys
import json
import glob
import logging
import argparse
import pkg_resources

from tronn.util.pwms import MotifSetManager # TODO move this?
from tronn.util.scripts import setup_run_logs
from tronn.util.scripts import parse_multi_target_selection_strings

def parse_args():
    """Prepare argument parser. Main subcommands are set up here first
    """
    parser = argparse.ArgumentParser(
        description='TRONN: Transcriptional Regulation Optimized Neural Nets')
    subparsers = parser.add_subparsers(dest='subcommand_name')

    # command for preprocessing data
    add_preprocess_parser(subparsers)
    
    # command for training
    add_train_parser(subparsers)

    # command for evaluation
    add_evaluate_parser(subparsers)

    # command for prediction
    add_predict_parser(subparsers)
    
    # command for motif scan
    add_scanmotifs_parser(subparsers)

    # command for dmim scan
    add_dmim_parser(subparsers)
    
    # command for network analysis
    add_buildgrammars_parser(subparsers)

    # command for synergy scan
    add_synergy_parser(subparsers)
    
    # command for activation maximization
    add_dream_parser(subparsers)

    # command for rational design with sig motifs
    # add_design_parser(subparsers)
    
    # TODO command for variant predictions
    add_analyzevariants_parser(subparsers)
    
    # baseline model
    add_baseline_parser(subparsers)

    # visualize importance scores parser
    add_plotregions_parser(subparsers)
    
    # command for extracting model variables
    add_export_model_parser(subparsers)
    
    # parse args
    args = parser.parse_args()

    return args


def _add_output_args(parser, out_dir="./"):
    """Add output directory and prefix args to parser
    """
    parser.add_argument(
        "-o", "--out_dir", dest="out_dir", type=str,
        default=out_dir,
        help = "Output directory (default: current)")
    parser.add_argument(
        '--prefix', required=True,
        help='prefix to attach onto file names')

    return None


def _setup_annotations(args):
    """load annotation files from json and replace the json
    file with the dictionary
    """
    with open(args.annotations, "r") as fp:
        annotations = json.load(fp)
    args.annotations = annotations
    
    return None


def _parse_files(dataset_key_strings):
    """given an arg string list, parse out into a dict
    assumes a format of: key=file1,file2,..::param1=val,param2=val,...
    """
    if dataset_key_strings is None:
        return {}

    key_dict = {}
    for dataset_key_string in dataset_key_strings:
        # first split on "::" then on "="
        key_and_params = dataset_key_string.split("::")
        
        # set up key and key items (filenames or indices)
        key_string = key_and_params[0]
        key_and_items = key_string.split("=")
        key = key_and_items[0]
        if len(key_and_items) == 1:
            key_items = []
        else:
            item_string = key_and_items[1]
            key_items = [
                int(val) if _is_integer(val) else val
                for val in item_string.split(",")]
            
        # set up params
        if len(key_and_params) == 1:
            params = {}
        else:
            param_string = key_and_params[1]
            params = dict(
                [param.split("=")
                 for param in param_string.split(",")])
            
        # combine
        key_dict[key] = (key_items, params)
    
    return key_dict


def _setup_preprocess(args):
    """load preprocess files
    """
    args.labels = _parse_files(args.labels)
    args.signals = _parse_files(args.signals)

    return None


def add_preprocess_parser(subparsers):
    """Add data generation function argument parser
    """
    argparser_preprocess = subparsers.add_parser(
        "preprocess",
        help="Preprocess data into TRONN formats")

    # group for input files
    group_input = argparser_preprocess.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        "--annotations", required=True,
        help="json file of annotation files")
    group_input.add_argument(
        "--labels", nargs='+', required=True,
        help='list of key=files_separated_by_commas::params, files are in BED/narrowPeak format')
    group_input.add_argument(
        "--signals", nargs='+',
        help='list of key=files_separated_by_comma::params, files are in bigwig format')
    group_input.add_argument(
        "--master_label_keys", nargs="+", default=[],
        help="subset of label files to use to make master regions file")
    group_input.add_argument(
        "--master_bed_file", default=None,
        help="master regions file for all examples")

    # group for options
    group_opts = argparser_preprocess.add_argument_group(
        "Data generation options")
    group_opts.add_argument(
        "--rc", action="store_true",
        help='Reverse complement')
    group_opts.add_argument(
        "--genomewide", action="store_true",
        help='build examples across whole genome')
    group_opts.add_argument(
        "--parallel", default=12, type=int,
        help='Number of parallel threads to use')
    
    # group for output files
    group_output = argparser_preprocess.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="./datasets/dataset")
    group_output.add_argument(
        "--tmp_dir", help="directory for tmp files")

    return


def _add_dataset_args(parser):
    """add dataset args
    """
    parser.add_argument(
        "--data_format", default="hdf5",
        help="dataset storage format (hdf5, bed, custom)")
    parser.add_argument(
        "--data_dir",
        help="h5 file directory")
    parser.add_argument(
        "--data_files", nargs="+", default=[],
        help="dataset files")
    parser.add_argument(
        "--fasta",
        help="fasta file")
    parser.add_argument(
        "--dataset_json",
        help="use a json as input instead. other data args will supersede this json if requested")
    parser.add_argument(
        "--targets", nargs="+", default=[],
        help="which datasets to load as targets (ordered). Expected format: data_key=indices_list_with_commas")
    parser.add_argument(
        "--filter_targets", nargs="+", default=[],
        help="at least 1 of these targets must be POSITIVE. Format: data_key=indices_list_with_commas")
    parser.add_argument(
        "--singleton_filter_targets", nargs="+", default=[],
        help="2 or more of these targets must be POSITIVE. Format: data_key=indices_list_with_commas")
    parser.add_argument(
        "--target_indices", nargs="+", default=[], type=int,
        help="after collection of labels, which of those to finally select. Format: indices_list_with_commas")
    parser.add_argument(
        "--dataset_examples", nargs="+", default=[],
        help="option to throw in specific examples to run")
    parser.add_argument(
        "--fifo", action="store_true",
        help="don't shuffle data")

    return None


def _setup_dataset_args(args):
    """set up dataset options
    """
    # first load dataset json if it is given
    if args.dataset_json is not None:
        with open(args.dataset_json, "r") as fp:
            args.dataset_json = json.load(fp)
    else:
        args.dataset_json = {}

    # load data files from json if not given
    if len(args.data_files) == 0:
        args.data_files = args.dataset_json.get("data_files")
        # load data dir from json if not given
        if args.data_dir is None:
            args.data_dir = args.dataset_json.get("data_dir")
        
    # load targets from json if not given
    if len(args.targets) == 0:
        args.targets = args.dataset_json.get("targets", [])
    else:
        args.targets = parse_multi_target_selection_strings(
            args.targets)

    # load filter targets from json if not given
    if len(args.filter_targets) == 0:
        args.filter_targets = args.dataset_json.get("filter_targets", [])
    else:
        args.filter_targets = parse_multi_target_selection_strings(
            args.filter_targets)

    # if fasta is not given, check dataset json
    if args.fasta is None:
        args.fasta = args.dataset_json.get("fasta")
    
    return None


def _add_model_args(parser):
    """add model args
    """
    parser.add_argument(
        "--model_framework", default="tensorflow",
        help="deep learning framework (tensorflow(tronn), keras, pytorch)")
    parser.add_argument(
        "--model", nargs="+",
        help="net function")
    parser.add_argument(
        "--model_json",
        help="use json as input instead")
    parser.add_argument(
        "--model_dir",
        help="model directory (if you want to update path)")
    parser.add_argument(
        "--model_checkpoint",
        help="model checkpoint (if you want to update checkpoint)")
    parser.add_argument(
        "--logit_indices", nargs="+", default=[], type=int,
        help="which logits from net to load")
    parser.add_argument(
        "--num_gpus", default=1, type=int,
        help="how many GPUs are available")

    return None


def _setup_model_args(args):
    """set up model options that came in through cmd line
    assumes a format of: model_name param=val,param=val,...

    Returns:
      converts args.model to dict of all model inputs
    """
    # first set up everything through args.model
    if args.model is not None:
        model = {
            "name": args.model[0],
            "params": {}}
        for model_arg in args.model[1:]:
            if '=' in model_arg:
                # params
                key, value = model_arg.split('=', 1)
                # check if param is a list
                if "," in value:
                    model["params"][key] = value.split(",")
                else:
                    model["params"][key] = eval(value)
            else:
                # action store true
                model["params"][model_arg] = True
        args.model = model
    else:
        with open(args.model_json, "r") as fp:
            args.model = json.load(fp)

    # update checkpoint if needed
    if args.model_checkpoint is not None:
        args.model["checkpoint"] = args.model_checkpoint

    # and update for model json dir
    if args.model_json is not None:
        args.model["checkpoint"] = "{}/train/{}".format(
            os.path.dirname(args.model_json),
            os.path.basename(args.model["checkpoint"]))
        
    # update model dir (and checkpoint location) if needed
    if args.model_dir is not None:
        args.model["model_dir"] = args.model_dir
        if args.model.get("checkpoint") is not None:
            args.model["checkpoint"] = "{}/{}".format(
                args.model_dir,
                os.path.basename(args.model["checkpoint"]))

    # if model dir is still none, use out_dir
    if args.model.get("model_dir") is None:
        args.model["model_dir"] = args.out_dir
    
    return None


def _setup_warm_start_args(args):
    """set up warm start args
    """
    # first, extract model json if defined
    if args.transfer_model_json is not None:
        with open(args.transfer_model_json, "r") as fp:
            args.transfer_model = json.load(fp)
    else:
        args.transfer_model = {}
            
    # only update transfer checkpoint if it was None
    if args.transfer_checkpoint is None:
        args.transfer_checkpoint = args.transfer_model.get("checkpoint")

    # add skip vars to warm start params
    args.transfer_params = {"skip": args.transfer_skip_vars}
    
    return None


def add_train_parser(subparsers):
    """Add argument parser for training
    """
    cmd_argparser = subparsers.add_parser(
        "train",
        help="Train a NN model")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    
    # group for cross validation params
    group_cv = cmd_argparser.add_argument_group(
        "Cross validation params")
    group_cv.add_argument(
        "--full_train", action="store_true",
        help="ignore early stopping, train for full num epochs")
    group_cv.add_argument(
        "--kfolds", default=10, type=int,
        help="number of folds to split the data")
    group_cv.add_argument(
        "--valid_folds", nargs="+", default=[0], type=int,
        help="which fold to use as validation")
    group_cv.add_argument(
        "--test_folds", nargs="+", default=[1], type=int,
        help="which fold to use as test")

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)

    # group for warm starting
    group_transfer = cmd_argparser.add_argument_group(
        "Transfer model arguments")
    group_transfer.add_argument(
        "--transfer_model_json", default=None,
        help="transfer model json. must have a checkpoint")
    group_transfer.add_argument(
        "--transfer_checkpoint", default=None,
        help="transfer model checkpoint, if defined will supersede model json")
    group_transfer.add_argument(
        "--transfer_skip_vars", nargs="+", default=[],
        help="variable names to skip (note searches these as substrings of var names)")
    group_transfer.add_argument(
        "--use_transfer_splits", action="store_true",
        help="set up train/valid/test splits using transfer model info")
    
    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "Training arguments")
    group_params.add_argument(
        '--epochs', default=30, type=int,
        help='number of epochs')
    group_params.add_argument(
        '--batch_size', default=256, type=int,
        help='batch size')
    group_params.add_argument(
        '--early_stopping_metric', default='mean_auprc', type=str,
        help='metric to use for early stopping')
    group_params.add_argument(
        '--patience', default=3, type=int,
        help='metric to use for early stopping')
    group_params.add_argument(
        "--regression", action="store_true",
        help="run regression instead of classification")
    group_params.add_argument(
        "--distributed", action="store_true",
        help="run on multi gpus")
    group_params.add_argument(
        '--finetune_targets', nargs="+", default=[],
        help="tasks to finetune")

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='model')
    group_output.add_argument(
        "--get_dataset_metrics", action="store_true",
        help="calculate dataset metrics if needed")

    return


def add_evaluate_parser(subparsers):
    """Add argument parser for test-time evaluation
    """
    cmd_argparser = subparsers.add_parser(
        "evaluate",
        help="Evaluate model")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    
    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "Parameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
    	"--regression", action="store_true",
    	help="evaluate regression")
    group_params.add_argument(
    	"--num_evals", default=64000, type=int,
    	help="Number of steps to run for prediction")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='evaluation')
    
    return


def add_predict_parser(subparsers):
    """Add argument parser for predicting with trained models
    """
    cmd_argparser = subparsers.add_parser(
        "predict",
        help="Predict with trained model")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)

    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "Parameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
    	"--regression", action="store_true",
    	help="evaluate regression")
    group_params.add_argument(
    	"--num_evals", default=1000, type=int,
    	help="Number of steps to run for prediction")

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='prediction')
    
    return


def _add_interpretation_args(parser, inference_fn="sequence_to_motif_scores"):
    """set up trained model info
    """
    parser.add_argument(
        "--infer_json",
        help="inference json file to continue from last run")
    parser.add_argument(
        "--inference_targets", nargs="+", type=int,
        help="tasks to extract importance scores - must be global indices") 
    parser.add_argument(
        "--backprop", default="input_x_grad",
        help="what method to use for interpretation")
    parser.add_argument(
        "--inference_fn", default=inference_fn,
        help="inference stack")
    parser.add_argument(
        "--left_clip", default=420, type=int,
        help="how much to clip off the left sequence flank")
    parser.add_argument(
        "--right_clip", default=580, type=int,
        help="how much to clip off the right sequence flank")
    parser.add_argument(
        "--pwm_file", default=None,
        help="pwm file")
    parser.add_argument(
        "--sample_size", default=100000, type=int,
        help="total regions (post filtering) to return")
    parser.add_argument(
        "--prediction_sample", default=None,
        help="file of predictions for quantile norm")
    parser.add_argument(
        "--debug", action="store_true",
        help="visualize outputs to help debug. NOTE: use small sample size!")
    
    return


def _setup_interpretation_args(args):
    """given parsed args, adjust inputs as needed
    matches _add_interpretation_options
    """
    # load infer json if it exists
    if args.infer_json is not None:
        with open(args.infer_json, "r") as fp:
            args.infer_json = json.load(fp)
    else:
        args.infer_json = {}

    # load out of infer json if not overridden in cmd line
    if args.model_json is None:
        args.model_json = args.infer_json.get("model_json")
    if args.inference_targets is None:
        args.inference_targets = args.infer_json.get("inference_targets")
    if args.pwm_file is None:
        args.pwm_file = args.infer_json.get("pwm_file")
            
    # pwm file
    args.pwm_list = MotifSetManager.read_pwm_file(args.pwm_file)
    args.pwm_names = [pwm.name for pwm in args.pwm_list]
    args.pwm_dict = MotifSetManager.read_pwm_file(args.pwm_file, as_dict=True)

    # set up inference dict
    args.inference_params = {
        "cmd_name": args.subcommand_name,
        "inference_mode": True, # for estimator framework
        "model_reuse": True if args.model_framework == "tensorflow" else False,
        "inference_fn_name": args.inference_fn,
        "importance_task_indices": args.inference_targets,
        "backprop": args.backprop,
        "left_clip": args.left_clip,
        "right_clip": args.right_clip,
        "prediction_sample": args.prediction_sample,
        "pwms": args.pwm_list}
    
    return


def _add_mutate_args(parser):
    """add args for mutational analyses
    """
    parser.add_argument(
        "--mutate_type", default="shuffle",
        help="what kind of mutation to do (point or shuffle)")
    
    return

def _setup_mutate_args(args):
    """add in mutate args as needed
    """
    args.inference_params.update({"mutate_type": args.mutate_type})
    
    return


def _add_sig_pwms_args(parser):
    """add in args to use sig pwm vectors
    """
    parser.add_argument(
        "--sig_pwms_file",
        help="if this arg is used, pvals will be used from here and not from scan file")
    parser.add_argument(
        "--sig_pwms_key", default="pvals",
        help="which dataset group has the sig pwm vectors")

    return


def _add_visualization_args(parser):
    """set up visualization after interpretation options
    """
    parser.add_argument(
        "--visualize_R", nargs="+", default=[],
        help="visualize with R scripts")
    parser.add_argument(
        "--visualize_multikey_R", nargs="+", default=[],
        help="visualize with R scripts")
    
    return


def _setup_visualization_args(args):
    """given parsed args, adjust visualization inputs as needed
    matches _add_visualization_options
    """
    if args.visualize_R is not None:
        args.visualize_R = _parse_files(args.visualize_R)
    if args.visualize_multikey_R is not None:
        args.visualize_multikey_R = _parse_files(args.visualize_multikey_R)

    return


def add_scanmotifs_parser(subparsers):
    """Scan motifs
    """
    cmd_argparser = subparsers.add_parser(
        "scanmotifs",
        help="Scan sequence with motifs from a PWM file")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    # placeholder right now (to make downstream processing in interp equal)
    # figure out how to remove?
    group_dataset.add_argument( 
        "--processed_inputs", action="store_true",
        help="inputs are already preprocessed through some inference stack")

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=48, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_motif_scores") # adjust for defaults
    _add_visualization_args(group_interpret)
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="motifs")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory as needed')

    return


def add_buildgrammars_parser(subparsers):
    """build grammars from motif scores or mutational results
    """
    cmd_argparser = subparsers.add_parser(
        "buildgrammars",
        help="build grammars from scanning results (motif or mutational)")

    # group for type
    group_analysis = cmd_argparser.add_argument_group(
        "Grammar build type")
    group_analysis.add_argument(
        "--scan_type", default="dmim",
        help="type of results being used")
    group_analysis.add_argument(
        "--min_positive_tasks", default=2, type=int,
        help="minimum cell states (tasks) in which effect exists to generate edge")
    group_analysis.add_argument(
        "--subgraph_max_k", default=3, type=int,
        help="when searching graph, max nodes to include in a subgraph")
    group_analysis.add_argument(
        "--min_support_fract", default=0.10, type=float,
        help="minimum region fract that is required for a grammar (overruled by min support if min support is larger)")
    group_analysis.add_argument(
        "--min_support", default=200, type=int,
        help="min number of examples needed to support a grammar")
    #group_analysis.add_argument(
    #    "--max_overlap_fraction", default=0.3, type=float,
    #    help="max fraction of regions that can overlap between two subgraphs")
    #group_analysis.add_argument(
    #    "--return_top_k", type=int,
    #    help="if requested return just the top k subgraphs, ranked by output strength")

    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    group_dataset.add_argument(
        "--scan_file", required=True,
        help="h5 file of scan results")
    group_dataset.add_argument(
        "--foreground_targets", required=True,
        help="key to pvals")
    group_dataset.add_argument(
        "--aux_data_keys", nargs="+", default=[],
        help="extra dataset summaries to add")
    #group_dataset.add_argument(
    #    "--scan_key",
    #    help="dataset key with scores") # either motifs {N, task, M} or dmim {N, mutM, task, M}, with dmim need to consider outputs logits {N, mutM, logits}
    _add_sig_pwms_args(group_dataset)
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="grammars")
    
    return


def add_dmim_parser(subparsers):
    """scan grammars from a grammar file
    """
    cmd_argparser = subparsers.add_parser(
        "dmim",
        help="score sequences for grammars")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    group_dataset.add_argument(
        "--processed_inputs", action="store_true",
        help="inputs are already preprocessed through some inference stack") # deprecate, assume this is true
    _add_sig_pwms_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
        
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=8, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_dmim")
    _add_mutate_args(
        group_interpret)
    _add_visualization_args(group_interpret)

    # group for cmd specific functions
    group_other = cmd_argparser.add_argument_group(
        "Misc params")
    #group_other.add_argument(
    #    "--manifold_file", help="manifold h5 file")
    group_other.add_argument(
        "--foreground_targets", nargs="+",
        help="which targets to use as foreground for calling motif enrichment")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="dmim")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory')
    
    return


def add_synergy_parser(subparsers):
    """scan grammars from a grammar file
    """
    cmd_argparser = subparsers.add_parser(
        "synergy",
        help="score sequences for grammars")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    group_dataset.add_argument(
        "--processed_inputs", action="store_true",
        help="inputs are already preprocessed through some inference stack")
    
    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
        
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=8, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_synergy")
    _add_mutate_args(
        group_interpret)
    _add_visualization_args(group_interpret)

    # group for cmd specific functions
    group_other = cmd_argparser.add_argument_group(
        "Misc params")
    group_other.add_argument(
        "--grammar_file", help="grammar (gml) file")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="dmim")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory')
    
    return


def add_dream_parser(subparsers):
    """activation maximization
    """
    cmd_argparser = subparsers.add_parser(
        "dream",
        help="use activation maximization to generate novel synthetic sequences")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset) # use BED input? or array input

    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    _add_interpretation_args(group_interpret)

    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "dream hyperparameters")
    group_params.add_argument(
        "--max_iter", default=20, type=int,
        help="number of iterations for dreaming")
    group_params.add_argument(
        "--edit_dist", default=1000, type=int,
        help="edit distance from start position allowed")

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="dream")
    group_output.add_argument(
        "--plot", action="store_true",
        help="show development of sequence across time")

    return 


def add_analyzevariants_parser(subparsers):
    """analyze variants
    """
    cmd_argparser = subparsers.add_parser(
        "analyzevariants",
        help="Analyze variants from a VCF file")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    group_dataset.add_argument(
        "--data_format", default="vcf",
        help="format for dataloader")
    group_dataset.add_argument(
        "--vcf_file",
        help="VCF formatted input file of variants")
    group_dataset.add_argument(
        "--fasta",
        help="fasta file")

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)

    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=32, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="variants_to_scores")
    _add_visualization_args(group_interpret)
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="motifs")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory')

    
    return


def add_baseline_parser(subparsers):
    """Add train baseline function argument parser
    """
    argparser_baseline = subparsers.add_parser("baseline", help="Run baseline model")

    # group for input files
    group_input = argparser_baseline.add_argument_group("Input files and folders")
    group_input.add_argument('--data_dir', help='Data directory of kmer hdf5 files')
    group_input.add_argument(
        "--cvfold", default=0, type=int,
        help="which files to train on [0, 1, 2]")
    group_input.add_argument(
        "--num_classes", default=2, type=int,
        help="how many classes")
    group_input.add_argument(
        "--kmers", action="store_true",
        help="featurize as kmers")
    group_input.add_argument(
        "--kmer_len", default=6, type=int,
        help="what kmer len to featurize with")
    group_input.add_argument(
        "--motifs", action="store_true",
        help="featurize as kmers")
    group_input.add_argument(
        "--pwm_file",
        help="motif_file")
    group_input.add_argument(
        "--tasks", nargs="+", default=[], type=int,
        help="tasks to train on (default is all)")

    # group for parameters
    group_params = argparser_baseline.add_argument_group("Training hyperparameters")
    group_params.add_argument('--batch_size', default=128, type=int, help='batch size')
    group_params.add_argument('--num_trees', default=100, type=int, help='how many trees')
    group_params.add_argument('--max_nodes', default=10000, type=int, help='max nodes per tree')
    group_params.add_argument('--num_evals', default=1000, type=int, help='num evaluation batches')

    # group for output files
    group_output = argparser_baseline.add_argument_group("Output files and folders")
    _add_output_args(group_output, out_dir='baseline')

    return


def add_plotregions_parser(subparsers):
    """plot a region, either command line region ID or BED file of regions
    """
    cmd_argparser = subparsers.add_parser(
        "plotregions",
        help="plot importance scores for a region")

    # inputs
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    group_dataset.add_argument(
        "--fasta",
        help="fasta file")
    group_dataset.add_argument(
        "--region_id",
        help="single region to plot, in the form chrom:start-stop (ex chr1:100-200)")
    group_dataset.add_argument(
        "--bed_input",
        help="bed file of regions to plot")

    # params
    group_params = cmd_argparser.add_argument_group(
        "Param arguments")
    group_params.add_argument(
        "--stride", default=1, type=int,
        help="stride to get importance scores")
    
    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="grammars")
    
    return


def add_makebigwig_parser(subparsers):
    """go through genome and make a bigwig
    """
    
    

    return


def add_export_model_parser(subparsers):
    """Add argument parser for extracting trained params from a model
    """
    cmd_argparser = subparsers.add_parser(
        "exportmodel",
        help="extract model variables")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    group_model.add_argument(
        "--skip", nargs="+", default=[],
        help="substrings in variable names to skip")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='export')
    
    return


def _is_integer(val):
    """helper function for parsing
    """
    try:
        int(val)
        return True
    except ValueError:
        return False
    

def main():
    """Main function for running TRoNN functions
    """
    # parse args and set up
    args = parse_args()
    args.out_dir = os.path.abspath(args.out_dir)
    os.system("mkdir -p {}".format(args.out_dir))
    setup_run_logs(args, args.subcommand_name)

    # get subcommand run function and run
    subcommand = args.subcommand_name

    if subcommand == "preprocess":
        _setup_annotations(args)
        _setup_preprocess(args)
        from tronn.preprocess_cmd import run
        run(args)
    elif subcommand == 'train':
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_warm_start_args(args)
        from tronn.train_cmd import run
        run(args)
    elif subcommand == "evaluate":
        _setup_dataset_args(args)
        _setup_model_args(args)
        from tronn.evaluate_cmd import run
        run(args)
    elif subcommand == "predict":
        _setup_dataset_args(args)
        _setup_model_args(args)
        from tronn.predict_cmd import run
        run(args)
    elif subcommand == "scanmotifs":
        _setup_interpretation_args(args)
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_visualization_args(args)
        from tronn.scanmotifs_cmd import run
        run(args)
    elif subcommand == "dmim":
        _setup_interpretation_args(args)
        _setup_mutate_args(args)
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_visualization_args(args)
        from tronn.dmim_cmd import run
        run(args)
    elif subcommand == "buildgrammars":
        from tronn.buildgrammars_cmd import run
        run(args)
    elif subcommand == "synergy":
        _setup_interpretation_args(args)
        _setup_mutate_args(args)
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_visualization_args(args)
        from tronn.synergy_cmd import run
        run(args)
    elif subcommand == "dream":
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_interpretation_options(args)
        from tronn.dream_cmd import run
        run(args)
    elif subcommand == "analyzevariants":
        _setup_interpretation_args(args)
        _setup_model_args(args)
        from tronn.analyzevariants_cmd import run
        run(args)
    elif subcommand == "baseline":
        _setup_dataset_args(args)
        from tronn.run_tensorforest import run
        run(args)
    elif subcommand == "plotregions":
        _setup_model_args(args)
        from tronn.plotregions_cmd import run
        run(args)
    elif subcommand == "exportmodel":
        _setup_dataset_args(args)
        _setup_model_args(args)
        from tronn.export_cmd import run
        run(args)
    # add new commands here
        
    return None


if __name__ == '__main__':
    main()
