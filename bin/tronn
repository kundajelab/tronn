#!/usr/bin/env python

"""Description: TRONN main executable
"""

import os
import sys
import logging
import argparse
import glob
import pkg_resources
import json


def parse_args():
    """Prepare argument parser. Main subcommands are set up here first
    """
    parser = argparse.ArgumentParser(
        description='TRONN: Transcriptional Regulation Optimized Neural Nets')
    parser.add_argument(
        "--cluster", default="kundaje",
        help="what cluster you're on (get annotations)")
    subparsers = parser.add_subparsers(dest='subcommand_name')

    # command for preprocessing data
    add_preprocess_parser(subparsers)
    
    # command for training
    add_train_parser(subparsers)

    # command for evaluation
    add_evaluate_parser(subparsers)

    # command for prediction/scanning
    add_predict_parser(subparsers)

    # command for making motifs with wkm
    add_wkm_parser(subparsers)

    # command for grouping motifs
    add_bagmotifs_parser(subparsers)

    # command for in silico mutagenesis
    add_ism_parser(subparsers)

    # command for testing multitask interpretation
    add_multitask_interpretation_parser(subparsers)
    
    # TODO(dk) add in a function to do
    # all grammar work (group motifs, ISM)

    # findgrammar test bed
    add_findgrammars_parser(subparsers)
    
    # scangrammar test bed
    add_scangrammars_parser(subparsers)
    
    # get motifs test bed
    add_getmotifs_parser(subparsers)

    add_vizimpts_parser(subparsers)

    # load data jsons
    #load_data_files(parser)

    add_baseline_parser(subparsers)

    add_extract_params_parser(subparsers)

    add_preprocess_variants_parser(subparsers)
    
    # parse args
    args = parser.parse_args()

    return args


def load_data_files(parser):
    """Load all json files in so that we have handles on all data
    """
    json_files = glob.glob(
        pkg_resources.resource_filename('tronn', 'data/*.json'))
    data_files = {}
    
    for json_file in json_files:
        key_name = os.path.basename(json_file).split('.json')[0]
        with open(json_file, 'r') as fp:
            parser.add_argument(
                "--{}".format(key_name),
                type=dict, default=json.load(fp),
                help="Support data files")

    return


# TODO(dk) fix this to add more options to model param setup
def add_model_params(args):
    """Add model configs
    """
    # parse model configs
    model_config = {}
    model_config['name'] = args.model[0]
    for model_arg in args.model[1:]:
        if '=' in model_arg:
            name, value = model_arg.split('=', 1)
            model_config[name] = eval(value)
        else:
            model_config[model_arg] = True
    args.model = model_config

    return 


def add_output_options(parser, out_dir="./"):
    """Add an output directory and prefix if desired
    """
    parser.add_argument(
        "-o", "--out_dir", dest="out_dir", type=str,
        default=out_dir,
        help = "Output directory (default: current)")
    parser.add_argument(
        '--prefix', required=True,
        help='prefix to attach onto file names')

    return


def add_hdf5_data_options(parser):
    """Add data options for hdf5 files
    """
    parser.add_argument(
        "--data_dir", required=True,
        help="hdf5 file directory")
    parser.add_argument(
        "--tasks", nargs="+", default=[], type=int,
        help="tasks to train on (default is all)")
    
    
def add_preprocess_parser(subparsers):
    """Add data generation function argument parser
    """
    argparser_preprocess = subparsers.add_parser(
        "preprocess",
        help="Preprocess data into TRONN formats")

    # group for input files
    group_input = argparser_preprocess.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        "--labels", nargs='+', required=True,
        help='list of label file peak sets (BED/narrowPeak)')
    load_data_files(group_input)

    # group for options
    group_opts = argparser_preprocess.add_argument_group(
        "Data generation options")
    group_opts.add_argument(
        "--rc", action="store_true",
        help='Reverse complement')
    group_opts.add_argument(
        "--no_flank_negs", action="store_true",
        help="turn off flank negatives")
    group_opts.add_argument(
        "--no_dhs_negs", action="store_true",
        help="turn off flank negatives")
    group_opts.add_argument(
        "--random_negs", action="store_true",
        help="add random negatives")
    group_opts.add_argument(
        "--univ_neg_num", type=int,
        help='number of universal negatives to grab from univ DHS regions')
    group_opts.add_argument(
        "--parallel", default=12, type=int,
        help='Number of parallel threads to use')
    group_opts.add_argument(
        "--kmerize", action='store_true',
        help="generate kmer datasets also")
    
    # group for output files
    group_output = argparser_preprocess.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="dataset")

    return


def add_preprocess_variants_parser(subparsers):
    """Specifically preprocess a variants file to generate examples for prediction
    """
    argparser_preprocess = subparsers.add_parser(
        "preprocess_variants",
        help="Preprocess data into TRONN formats")

    # group for input files
    group_input = argparser_preprocess.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        "--variant_file", help="variant_file")
    load_data_files(group_input)

    # group for options
    group_opts = argparser_preprocess.add_argument_group(
        "Data generation options")
    group_opts.add_argument(
        "--parallel", default=12, type=int,
        help='Number of parallel threads to use')
    
    # group for output files
    group_output = argparser_preprocess.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="dataset")

    return


def add_train_parser(subparsers):
    """Add argument parser for training
    """
    argparser_train = subparsers.add_parser(
        "train",
        help="Train a TRONN model")
    
    # group for input files
    group_input = argparser_train.add_argument_group(
        "Input files and folders")
    add_hdf5_data_options(group_input)
    group_input.add_argument(
        "--cvfold", default=0, type=int,
        help="which files to train on [0, 1, 2]")
    group_input.add_argument(
        '--restore_model_dir', default=None,
        help='restore from last checkpoint in dir')
    group_input.add_argument(
        '--restore_model_checkpoint', default=None,
        help='restore from desired checkpoint')
    group_input.add_argument(
        '--transfer_model_dir', default=None,
        help='transfer from last checkpoint in dir')
    group_input.add_argument(
        '--transfer_model_checkpoint', default=None,
        help='transfer from desired checkpoint') 
    
    # group for model
    group_model = argparser_train.add_argument_group(
        "Model definition")
    group_model.add_argument(
        '--model', nargs='+', required=True,
        help='choose model and provide configs')
    
    # group for parameters
    group_params = argparser_train.add_argument_group(
        "Training hyperparameters")
    group_params.add_argument(
        '--epochs', default=20, type=int,
        help='number of epochs')
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
        '--metric', default='mean_auprc', type=str,
        help='metric to use for early stopping')
    group_params.add_argument(
        '--patience', default=2, type=int,
        help='metric to use for early stopping')
    group_params.add_argument(
        '--finetune_tasks', nargs="+", default=[],
        help="tasks to finetune")

    # group for output files
    group_output = argparser_train.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir='model')

    return


def add_trained_nn_model_options(parser, required=True):
    """Set up standard necessary args for a trained NN model
    """
    parser.add_argument(
        '--model', nargs='+', required=required,
        help='choose model and provide configs')
    parser.add_argument(
        '--model_dir', default=None,
        help='restore from last checkpoint')
    parser.add_argument(
        "--model_checkpoint", default=None,
        help="restore from specific checkpoint")
    
    return


def add_model_picker(parser):
    """Set up model picker arguments. This is used 
    for prediction/evaluation functions where you might want to test
    motif, grammar, or the full neural net
    """
    parser.add_argument(
        "--model_type", required=True, default="nn",
        help="what kind of model: nn, motif, grammar")
    
    # arguments for nn model
    add_trained_nn_model_options(parser, required=False)
    
    # arguments for motif model
    parser.add_argument(
        '--pwm_files', nargs="+", default=None,
        help='list of PWM files (all will be loaded and run')

    # arguments for grammar model
    parser.add_argument(
        '--grammar_files', nargs="+",
        help='list of grammar files (all will be loaded and run')
    
    return


def add_evaluate_parser(subparsers):
    """Add argument parser for test-time evaluation
    """
    argparser_test = subparsers.add_parser(
        "evaluate",
        help="Evaluate TRONN model")

    # group for input files
    group_input = argparser_test.add_argument_group(
        "Input files and folders")
    add_hdf5_data_options(group_input)
    group_input.add_argument(
        "--fake_task_num", default=0, type=int,
        help="extra tasks to add (to match a smaller output dataset to larger model)")
    group_input.add_argument(
        "--cvfold", default=0, type=int,
        help="which files to train on [0, 1, 2]")

    # group for model
    group_model = argparser_test.add_argument_group(
        "Model definition")
    add_model_picker(group_model)
    
    # group for parameters
    group_params = argparser_test.add_argument_group(
        "Parameters")
    group_params.add_argument(
    	"--num_evals", default=512000, type=int,
    	help="Number of steps to run for prediction")
    group_params.add_argument(
    	"--reconstruct_regions", action="store_true",
    	help="Produce files where values are merged")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
        "--single_task", default=None, type=int,
        help="Evaluate ALL predictions on ONE task (give idx here)")
    
    # group for output files
    group_output = argparser_test.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir='evaluation')
    
    return


def add_predict_parser(subparsers):
    """Add argument parser for predicting with trained models
    """
    argparser_predict = subparsers.add_parser(
        "predict",
        help="Predict with trained model")

    # group for input files
    group_input = argparser_predict.add_argument_group(
        "Input files and folders")
    #add_hdf5_data_options(group_input)
    group_input.add_argument(
        "--data_dir", required=False,
        help="hdf5 file directory")
    group_input.add_argument(
        "--tasks", nargs="+", default=[], type=int,
        help="tasks to train on (default is all)")

    group_input.add_argument(
        "--input_bed", default=None,
        help="give an input BED file for prediction")
    group_input.add_argument(
        "--input_labels", nargs='+', default=[],
        help='list of label file peak sets (BED/narrowPeak). MUST MATCH INITIAL LABEL SET')
    load_data_files(group_input)

    # group for model
    group_model = argparser_predict.add_argument_group(
        "Model definition")
    add_model_picker(group_model)
    
    # group for parameters
    group_params = argparser_predict.add_argument_group(
        "Parameters")
    group_params.add_argument(
    	"--num_evals", default=1000, type=int,
    	help="Number of steps to run for prediction")
    group_params.add_argument(
    	"--reconstruct_regions", action="store_true",
    	help="Produce files where values are merged")
    group_params.add_argument(
    	"--variants", action="store_true",
    	help="Inputs are variants")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
        "--single_task", default=None, type=int,
        help="Get ALL predictions on ONE task (give idx here)")

    # group for output files
    group_output = argparser_predict.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir='prediction')
    
    return


def add_wkm_parser(subparsers):
    """Add argument parser for making motifs from weighted kmers
    """
    argparser_wkm = subparsers.add_parser(
        "makemotifs",
        help="Make motifs from weighted kmers")

    # group for input files
    group_input = argparser_wkm.add_argument_group(
    	"Input files and folders")
    group_input.add_argument(
        "--importance_files", nargs="+", required=True,
        help="Importance files (ideally thresholded) to extract kmers")
    
    # group for output files
    group_output = argparser_wkm.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="motifs")
    
    return


def add_bagmotifs_parser(subparsers):
    """Add argument parser for scanning motifs and clustering into motif bags
    """
    argparser_bagmotifs = subparsers.add_parser(
        "bagmotifs",
        help="Cluster motifs into groups")

    # group for input files
    group_input = argparser_bagmotifs.add_argument_group(
    	"Input files and folders")
    group_input.add_argument(
        "--importance_file", required=True,
        help="Importance files (ideally thresholded) to extract kmers")
    group_input.add_argument(
    	"--task_num", required=True, type=int,
    	help="Task number currently running (to get correct importance scores)")
    group_input.add_argument(
        "--motif_file", required=True,
        help="motif file for scanning")

    # group for parameters
    group_params = argparser_bagmotifs.add_argument_group(
        "Interpretation hyperparameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    
    # group for output files
    group_output = argparser_bagmotifs.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="motifclusters")
    
    return


def add_ism_parser(subparsers):
    """Add argument parser for extracting dependencies using
    in silico mutagenesis (ISM)
    """
    argparser_ism = subparsers.add_parser(
        "ism",
        help="Extract dependencies using ISM")

    # group for input files
    group_input = argparser_ism.add_argument_group(
    	"Input files and folders")
    group_input.add_argument(
        "--bed_dir", required=True,
        help="Directory of BED files")
    group_input.add_argument(
    	"--motif_sets_file", required=True,
    	help="Motif sets file")
    group_input.add_argument(
        "--pwm_file", required=True,
        help="file of PWMs")
    group_input.add_argument(
        '--tasks', nargs='+', default=[], type=int,
        help='tasks over which to get importances')
    load_data_files(group_input)

    # group for model
    group_model = argparser_ism.add_argument_group(
        "Model definition")
    add_trained_nn_model_options(group_model)

    # group for params
    group_params = argparser_ism.add_argument_group(
        "Interpretation hyperparameters")
    group_params.add_argument(
        '--batch_size', default=1, type=int,
        help='batch size')

    # group for output files
    group_output = argparser_ism.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="ism")

    return


def add_multitask_interpretation_parser(subparsers):
    """Testing multitask interpretation
    """
    argparser_multitask = subparsers.add_parser(
        "interpret_multitask",
        help="Run multitask interpretation")

    # group for input files
    group_input = argparser_multitask.add_argument_group(
    	"Input files and folders")
    add_hdf5_data_options(group_input)
    group_input.add_argument(
        "--importances_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")
    group_input.add_argument(
        "--interpretation_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")

    # group for model
    group_model = argparser_multitask.add_argument_group(
        "Model definition")
    add_trained_nn_model_options(group_model)

    # group for parameters
    group_params = argparser_multitask.add_argument_group(
        "Interpretation hyperparameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
        '--sample_size', default=220000, type=int,
        help='number of regions to get importance scores')

    # group for output files
    group_output = argparser_multitask.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="importances")
    group_output.add_argument(
        '--tmp_dir', default='importances_tmp',
        help='temporary scratch directory')
    group_output.add_argument(
        '--plot_samples', action='store_true',
        help='plot some sample sequences weighted by importance scores')

    return


def add_findgrammars_parser(subparsers):
    """
    """
    argparser_multitask = subparsers.add_parser(
        "findgrammars",
        help="Get grammars")
    
    # group for input files
    group_input = argparser_multitask.add_argument_group(
    	"Input files and folders")
    add_hdf5_data_options(group_input)
    group_input.add_argument(
        "--importances_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")
    group_input.add_argument(
        "--interpretation_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")

    # add in PWM information
    group_input.add_argument(
        "--pwm_file", help="pwm_file")
    group_input.add_argument(
        "--pwm_metadata_file", help="pwm_metadata_file")
    group_input.add_argument(
        "--motif_dir", help="where the motif files are")
    
    # group for model
    group_model = argparser_multitask.add_argument_group(
        "Model definition")
    add_trained_nn_model_options(group_model)
    group_model.add_argument(
        "--backprop", default="input_x_grad",
        help="what method to use for interpretation")
    group_model.add_argument(
        "--inference_fn", help="inference stack")

    # group for parameters
    group_params = argparser_multitask.add_argument_group(
        "Interpretation hyperparameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
        '--sample_size', default=220000, type=int,
        help='number of regions to get importance scores')

    # group for output files
    group_output = argparser_multitask.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="importances")
    group_output.add_argument(
        '--tmp_dir', default='importances_tmp',
        help='temporary scratch directory')
    group_output.add_argument(
        '--plot_samples', action='store_true',
        help='plot some sample sequences weighted by importance scores')

    return


def add_scangrammars_parser(subparsers):
    """scan grammars from a grammar file
    """
    argparser_multitask = subparsers.add_parser(
        "scangrammars",
        help="score sequences for grammar presence")
    
    # group for input files
    group_input = argparser_multitask.add_argument_group(
    	"Input files and folders")
    add_hdf5_data_options(group_input)
    group_input.add_argument(
        "--importances_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")
    group_input.add_argument( # TODO decide, need this or not?
        "--interpretation_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")

    # add in grammar information
    group_input.add_argument(
        "--pwm_file", help="pwm_file")
    group_input.add_argument(
        "--pwm_metadata_file", help="pwm_metadata_file")
    group_input.add_argument(
        "--motif_dir", help="where the motif files are") # do i need this?
    group_input.add_argument(
        "--grammar_file", help="pwm_file")
    
    # group for model
    group_model = argparser_multitask.add_argument_group(
        "Model definition")
    add_trained_nn_model_options(group_model)
    group_model.add_argument(
        "--backprop", help="what method to use for interpretation")
    group_model.add_argument(
        "--inference_fn", help="inference stack")

    # group for parameters
    group_params = argparser_multitask.add_argument_group(
        "Interpretation hyperparameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
        '--sample_size', default=220000, type=int,
        help='number of regions to get importance scores')

    # group for output files
    group_output = argparser_multitask.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="importances")
    group_output.add_argument(
        '--tmp_dir', default='importances_tmp',
        help='temporary scratch directory')
    group_output.add_argument(
        '--plot_samples', action='store_true',
        help='plot some sample sequences weighted by importance scores')

    return


def add_getmotifs_parser(subparsers):
    """
    """
    argparser_multitask = subparsers.add_parser(
        "getmotifs",
        help="Get statistically significant motifs")
    
    # group for input files
    group_input = argparser_multitask.add_argument_group(
    	"Input files and folders")
    add_hdf5_data_options(group_input)
    group_input.add_argument(
        "--importances_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")
    group_input.add_argument(
        "--interpretation_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")

    # add in PWM information
    group_input.add_argument(
        "--pwm_file", help="pwm_file")
    
    # group for model
    group_model = argparser_multitask.add_argument_group(
        "Model definition")
    add_trained_nn_model_options(group_model)
    group_model.add_argument(
        "--backprop", help="what method to use for interpretation")

    # group for parameters
    group_params = argparser_multitask.add_argument_group(
        "Interpretation hyperparameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
        '--sample_size', default=220000, type=int,
        help='number of regions to get importance scores')

    # group for output files
    group_output = argparser_multitask.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="importances")
    group_output.add_argument(
        '--tmp_dir', default='importances_tmp',
        help='temporary scratch directory')
    group_output.add_argument(
        '--plot_samples', action='store_true',
        help='plot some sample sequences weighted by importance scores')

    return


def add_vizimpts_parser(subparsers):
    """
    """
    argparser_multitask = subparsers.add_parser(
        "vizimportances",
        help="Get back importance score visualizations for a region")


    # group for input files
    # start with BED file, preprocess, then run through
    group_input = argparser_multitask.add_argument_group(
    	"Input files and folders")
    load_data_files(group_input)
    group_input.add_argument(
        "--labels", nargs='+', required=True,
        help='list of label file peak sets (BED/narrowPeak)')
    group_input.add_argument(
        "--importances_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")
    
    # group for model
    group_model = argparser_multitask.add_argument_group(
        "Model definition")
    add_trained_nn_model_options(group_model)

    # group for parameters
    group_params = argparser_multitask.add_argument_group(
        "Interpretation hyperparameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')

    # group for output files
    group_output = argparser_multitask.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="importances")
    group_output.add_argument(
        '--tmp_dir', default='importances_tmp',
        help='temporary scratch directory')
    group_output.add_argument(
        '--plot_samples', action='store_true',
        help='plot some sample sequences weighted by importance scores')

    return



def add_baseline_parser(subparsers):
    """Add train baseline function argument parser
    """
    argparser_baseline = subparsers.add_parser("baseline", help="Run baseline model")

    # group for input files
    group_input = argparser_baseline.add_argument_group("Input files and folders")
    group_input.add_argument('--data_dir', help='Data directory of kmer hdf5 files')
    group_input.add_argument(
        "--cvfold", default=0, type=int,
        help="which files to train on [0, 1, 2]")
    group_input.add_argument(
        "--num_classes", default=2, type=int,
        help="how many classes")
    group_input.add_argument(
        "--kmers", action="store_true",
        help="featurize as kmers")
    group_input.add_argument(
        "--kmer_len", default=6, type=int,
        help="what kmer len to featurize with")
    group_input.add_argument(
        "--motifs", action="store_true",
        help="featurize as kmers")
    group_input.add_argument(
        "--pwm_file",
        help="motif_file")
    group_input.add_argument(
        "--tasks", nargs="+", default=[], type=int,
        help="tasks to train on (default is all)")

    # group for parameters
    group_params = argparser_baseline.add_argument_group("Training hyperparameters")
    group_params.add_argument('--batch_size', default=128, type=int, help='batch size')
    group_params.add_argument('--num_trees', default=100, type=int, help='how many trees')
    group_params.add_argument('--max_nodes', default=10000, type=int, help='max nodes per tree')
    group_params.add_argument('--num_evals', default=1000, type=int, help='num evaluation batches')

    # group for output files
    group_output = argparser_baseline.add_argument_group("Output files and folders")
    add_output_options(group_output, out_dir='baseline')

    return


def add_extract_params_parser(subparsers):
    """Add argument parser for extracting trained params from a model
    """
    argparser_extractparams = subparsers.add_parser(
        "extractparams",
        help="extract_params")

    # group for input files
    group_input = argparser_extractparams.add_argument_group(
        "Input files and folders")
    add_hdf5_data_options(group_input)

    # group for model
    group_model = argparser_extractparams.add_argument_group(
        "Model definition")
    add_model_picker(group_model)
    
    # group for parameters
    group_params = argparser_extractparams.add_argument_group(
        "Parameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    
    # group for output files
    group_output = argparser_extractparams.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir='params')
    
    return



def track_runs(args):
    """track command and github commit
    """
    # keeps track of restores (or different commands) in folder
    num_restores = len(glob.glob('{0}/{1}.command'.format(args.out_dir, args.subcommand_name)))
    logging_file = '{0}/{1}.command_{2}.log'.format(args.out_dir, args.subcommand_name, num_restores)
    
    # track github commit
    git_repo_path = os.path.dirname(os.path.realpath(__file__))
    os.system('echo "commit:" > {0}'.format(logging_file))
    os.system('git --git-dir={0}/.git rev-parse HEAD >> {1}'.format(
        git_repo_path.split("/bin")[0], logging_file))
    os.system('echo "" >> {0}'.format(logging_file))
    
    # write out the command
    with open(logging_file, 'a') as f:
        f.write(' '.join(sys.argv)+'\n\n')
    
    return logging_file


def main():
    """Main function for running TRoNN functions
    """
    args = parse_args()
    print "out_dir: %s" % args.out_dir
    os.system("mkdir -p {}".format(args.out_dir))
    logging_file = track_runs(args)
    #logging.basicConfig(filename=logging_file, level=logging.INFO)

    # get subcommand run function and run
    subcommand = args.subcommand_name

    if subcommand == "preprocess":
        args.annotations = args.annotations[args.cluster]
        from tronn.run_preprocess import run
        run(args)
    elif subcommand == "preprocess_variants":
        from tronn.run_preprocess import run_variants
        run_variants(args)
    elif subcommand == 'train':
        add_model_params(args)
        print 'model args: %s' % args.model
        from tronn.run_train import run
        run(args)
    elif subcommand == "evaluate":
        if args.model_type == "nn":
            add_model_params(args)
        print "model args: %s" % args.model
        from tronn.run_evaluate import run
        run(args)
    elif subcommand == "predict":
    	if args.model_type == "nn":
            add_model_params(args)
            print "model args: %s" % args.model
        from tronn.run_predict import run
        run(args)
    elif subcommand == "makemotifs": # this is modisco - TODO (on kmers)
        from tronn.run_wkm import run
        run(args)
    elif subcommand == "bagmotifs": # this... could be deleted?
    	from tronn.run_bagmotifs import run
    	run(args)
    elif subcommand == "ism": # this checks dependencies
    	add_model_params(args)
    	from tronn.run_ism import run
    	run(args)
    elif subcommand == "interpret_multitask": # this is modisco on seqlets - merge with makemotifs with option of kmer vs seqlet
        add_model_params(args)
        from tronn.run_multitask_interpretation import run
        run(args)
    elif subcommand == "findgrammars":
        add_model_params(args)
        from tronn.run_findgrammars import run
        run(args)
    elif subcommand == "scangrammars":
        add_model_params(args)
        from tronn.run_scangrammars import run
        run(args)
    elif subcommand == "getmotifs": # sorta defunct but keep to do HOMER style differential motifs
        add_model_params(args)
        from tronn.run_getmotifs import run
        run(args)
    elif subcommand == "baseline":
        from tronn.run_tensorforest import run
        run(args)
    elif subcommand == "extractparams":
        add_model_params(args)
        from tronn.util.extract_params import run
        run(args)
    elif subcommand == "vizimportances": # this needs to be cleaned up?
        args.annotations = args.annotations[args.cluster]
        add_model_params(args)
        from tronn.run_vizimpts import run
        run(args)
        
    return None


if __name__ == '__main__':
    main()
