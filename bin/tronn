#!/usr/bin/env python

"""Description: TRONN main executable
"""

import os
import sys
import logging
import subprocess
import argparse
import glob
import pkg_resources
import json


def parse_args():
    """Prepare argument parser. Main subcommands are set up here first
    """
    parser = argparse.ArgumentParser(
        description='TRONN: Transcriptional Regulation Optimized Neural Nets')
    subparsers = parser.add_subparsers(dest='subcommand_name')

    # command for preprocessing data
    add_preprocess_parser(subparsers)
    
    # command for training
    add_train_parser(subparsers)

    # command for evaluation
    add_evaluate_parser(subparsers)

    # command for prediction
    add_predict_parser(subparsers)

    # command for extracting importances
    add_importances_parser(subparsers)

    # command for making motifs with wkm
    add_wkm_parser(subparsers)

    # command for ISM
    add_ism_parser(subparsers)


    # OLD PARSERS BELOW TO BE DEPRECATED
    
    # command for interpretation
    add_interpret_parser(subparsers)

    # command for interpretation with kmers
    add_interpret_parser_2(subparsers)

    # command for scanning grammars
    add_scan_grammars_parser(subparsers)

    # command for running a baseline model (on kmers)
    add_baseline_parser(subparsers)
    
    # TODO consider a command for visualizing some example sequences


    load_data_files(parser)


    # parse args
    args = parser.parse_args()

    return args


def load_data_files(parser):
    """Load all json files in so that we have handles on all data
    """
    json_files = glob.glob(pkg_resources.resource_filename('tronn', 'data/*.json'))
    data_files = {}
    
    for json_file in json_files:
        key_name = os.path.basename(json_file).split('.json')[0]
        with open(json_file, 'r') as fp:
            parser.add_argument("--{}".format(key_name), type=dict, default=json.load(fp),
                                help="Support data files")

    return


def add_model_config(args):
    """Add model configs
    """
    # parse model configs
    model_config = {}
    model_config['name'] = args.model[0]
    for model_arg in args.model[1:]:
        if '=' in model_arg:
            name, value = model_arg.split('=', 1)
            model_config[name] = eval(value)
        else:
            model_config[model_arg] = True
    args.model = model_config

    return 


def add_out_dir_option(parser, default="./"):
    """Add an output directory if defined
    """
    parser.add_argument(
        "-o", "--out_dir", dest="out_dir", type=str, default=default,
        help = "Output directory (default: current)")

    return


def add_preprocess_parser(subparsers):
    """Add data generation function argument parser
    """
    argparser_preprocess = subparsers.add_parser(
        "preprocess",
        help="Preprocess data into TRONN formats")

    # group for input files
    group_input = argparser_preprocess.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        "--labels", nargs='+', required=True,
        help='list of label file peak sets (BED/narrowPeak)')

    # group for options
    group_opts = argparser_preprocess.add_argument_group(
        "Data generation options")
    group_opts.add_argument(
        "--rc",
        help='Reverse complement')
    group_opts.add_argument(
        "--univ_neg_num", default=200000,
        help='number of universal negatives to grab from univ DHS regions')
    group_opts.add_argument(
        "--parallel", default=12, type=int,
        help='Number of parallel threads to use')
    group_opts.add_argument(
        "--kmerize", action='store_true',
        help="generate kmer datasets also")
    
    # group for output files
    group_output = argparser_preprocess.add_argument_group(
        "Output files and folders")
    add_out_dir_option(group_output)
    group_output.add_argument(
        '--prefix', required=True,
        help='prefix to attach onto file names')

    return


def add_train_parser(subparsers):
    """Add argument parser for training
    """
    argparser_train = subparsers.add_parser(
        "train",
        help="Train a TRONN model")
    
    # group for input files
    group_input = argparser_train.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        '--data_dir', required=True,
        help='hdf5 file directory')
    group_input.add_argument(
        '--restore_model_dir', default=None,
        help='restore from last checkpoint')
    group_input.add_argument(
        '--transfer_model_dir', default=None,
        help='directory with same model to transfer') 
    group_input.add_argument(
        '--tasks', nargs='+', default=[], type=int,
        help='tasks over which to train multitask model on')
    
    # group for model
    group_model = argparser_train.add_argument_group(
        "Model definition")
    group_model.add_argument(
        '--model', nargs='+', required=True,
        help='choose model and provide configs')
    
    # group for parameters
    group_params = argparser_train.add_argument_group(
        "Training hyperparameters")
    group_params.add_argument(
        '--epochs', default=20, type=int,
        help='number of epochs')
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
        '--metric', default='mean_auprc', type=str,
        help='metric to use for early stopping')
    group_params.add_argument(
        '--patience', default=2, type=int,
        help='metric to use for early stopping')

    # group for output files
    group_output = argparser_train.add_argument_group(
        "Output files and folders")
    add_out_dir_option(group_output, default='log')
    group_output.add_argument(
        '--prefix', required=True,
        help='prefix to attach onto file names')

    return


def add_evaluate_parser(subparsers):
    """Add argument parser for test-time evaluation
    """
    argparser_test = subparsers.add_parser(
        "evaluate",
        help="Evaluate TRONN model")

    # TODO(dk) fill out arguments
    
    return


def add_predict_parser(subparsers):
    """Add argument parser for predicting with trained models
    """
    argparser_predict = subparsers.add_parser(
        "predict",
        help="Predict with trained model")
    
    return


def add_importances_parser(subparsers):
    """Add argument parser for getting feature importances
    """
    argparser_importances = subparsers.add_parser(
        "extractimportances",
        help="Get basepair level importance scores")

    # group for input files
    group_input = argparser_importances.add_argument_group(
    	"Input files and folders")
    group_input.add_argument(
        '--data_dir', required=True,
        help='hdf5 file directory')
    group_input.add_argument(
        '--model_dir', required=True,
        help='trained model (tensorflow checkpoint)')
    group_input.add_argument(
        '--tasks', nargs='+', default=[], type=int,
        help='tasks over which to get importances')

    # group for model
    group_model = argparser_importances.add_argument_group(
        "Model definition")
    group_model.add_argument(
        '--model', nargs='+', required=True,
        help='choose model and provide configs')

    # group for parameters
    group_params = argparser_importances.add_argument_group(
        "Interpretation hyperparameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
        '--sample_size', default=220000, type=int,
        help='number of regions to get importance scores')

    # group for output files
    group_output = argparser_importances.add_argument_group(
        "Output files and folders")
    add_out_dir_option(group_output)
    group_output.add_argument(
        '--prefix', default='tronn',
        help='prefix to attach onto file names')
    group_output.add_argument(
        '--tmp_dir', default='./',
        help='temporary scratch directory')
    group_output.add_argument(
        '--plot_samples', action='store_true',
        help='plot some sample sequences weighted by importance scores')
    
    return

def add_wkm_parser(subparsers):
    """Add argument parser for making motifs from weighted kmers
    """
    argparser_wkm = subparsers.add_parser(
        "makemotifs",
        help="Make motifs from weighted kmers")

    # group for input files
    group_input = argparser_wkm.add_argument_group(
    	"Input files and folders")
    group_input.add_argument(
        "--importance_files", nargs="+", required=True,
        help="Importance files (ideally thresholded) to extract kmers")

    
    

    
    return


def add_ism_parser(subparsers):
    """Add argument parser for extracting dependencies using
    in silico mutagenesis (ISM)
    """
    argparser_ism = subparsers.add_parser(
        "ism",
        help="Extract dependencies using ISM")
    

    return


# OLD PARSERS BELOW TO BE DEPRECATED

def add_interpret_parser(subparsers):
    """Add interpretation function argument parser
    """
    argparser_interpret = subparsers.add_parser("interpret", help="Interpret a TRoNN model")

    # group for input files
    group_input = argparser_interpret.add_argument_group("Input files and folders")
    group_input.add_argument('--data_dir', help='hdf5 file directory')
    group_input.add_argument('--model_dir', help='trained model for interpretation')
    group_input.add_argument('--tasks', nargs='+', default=[], type=int,
                             help='tasks over which to train multitask model on')

    # group for model
    group_model = argparser_interpret.add_argument_group("Model definition")
    group_model.add_argument('--model', nargs='+', help='choose model and provide configs')

    # group for parameters
    group_params = argparser_interpret.add_argument_group("Interpretation hyperparameters")
    group_params.add_argument('--batch_size', default=128, type=int, help='batch size')
    group_params.add_argument('--sample_size', default=220000, type=int,
                              help='number of regions to get importance scores')

    # group for annotation files
    group_data = argparser_interpret.add_argument_group("External data files")
    group_data.add_argument('--annotations', help='json file of external data files')

    # group for output files
    group_output = argparser_interpret.add_argument_group("Output files and folders")
    add_out_dir_option(group_output)
    group_output.add_argument('--prefix', default='tronn', help='prefix to attach onto file names')
    group_output.add_argument('--scratch_dir', help='temporary scratch directory')
    group_output.add_argument('--plot_importances', action='store_true',
                              help='plot some sample sequences weighted by importance scores')
    
    return


def add_interpret_parser_2(subparsers):
    """Add interpretation function argument parser
    """
    argparser_interpret = subparsers.add_parser("interpret2", help="Interpret a TRoNN model using wkm")

    # group for input files
    group_input = argparser_interpret.add_argument_group("Input files and folders")
    group_input.add_argument('--data_dir', help='hdf5 file directory')
    group_input.add_argument('--model_dir', help='trained model for interpretation')
    group_input.add_argument('--tasks', nargs='+', default=[], type=int,
                             help='tasks over which to train multitask model on')

    # group for model
    group_model = argparser_interpret.add_argument_group("Model definition")
    group_model.add_argument('--model', nargs='+', help='choose model and provide configs')

    # group for parameters
    group_params = argparser_interpret.add_argument_group("Interpretation hyperparameters")
    group_params.add_argument('--batch_size', default=128, type=int, help='batch size')
    group_params.add_argument('--sample_size', default=220000, type=int,
                              help='number of regions to get importance scores')

    # group for annotation files
    group_data = argparser_interpret.add_argument_group("External data files")
    group_data.add_argument('--annotations', help='json file of external data files')
    group_data.add_argument('--preprocess_annotations', help='json file of preprocess annotations')

    # group for output files
    group_output = argparser_interpret.add_argument_group("Output files and folders")
    add_out_dir_option(group_output)
    group_output.add_argument('--prefix', default='tronn', help='prefix to attach onto file names')
    group_output.add_argument('--scratch_dir', help='temporary scratch directory')
    group_output.add_argument('--plot_importances', action='store_true',
                              help='plot some sample sequences weighted by importance scores')
    
    return


def add_scan_grammars_parser(subparsers):
    """Add grammar scanning function argument parser
    """
    argparser_scan = subparsers.add_parser("scangrammars", help="Scan genomic regions for grammars")

    # group for input files
    group_input = argparser_scan.add_argument_group("Input files and folders")
    group_input.add_argument('--regions', help='input BED file')
    group_input.add_argument('--motifs', help='PWM file')
    group_input.add_argument('--grammars', help='Grammar file (must use names that match PWM file')
    group_input.add_argument('--labels', help='subset of --regions. Used to calculate metrics for you')
    group_input.add_argument("--annotations_json", help='json of support files')
    
    # group for options
    group_opts = argparser_scan.add_argument_group("Scanning options")
    group_opts.add_argument('--batch_size', default=128, type=int, help='batch size')
    group_opts.add_argument("--rc", action='store_true', help='Reverse complement')
    group_opts.add_argument("--univ_neg_num", default=0, help='number of universal negatives to grab from univ DHS regions')
    group_opts.add_argument("--parallel", default=24, type=int,
                            help='Number of parallel threads to use')

    # group for output files
    group_output = argparser_scan.add_argument_group("Output files and folders")
    add_out_dir_option(group_output)
    group_output.add_argument('--prefix', default='tronn', help='prefix to attach onto file names')
    group_output.add_argument('--scratch_dir', help='temporary scratch directory')

    return


def add_baseline_parser(subparsers):
    """Add train baseline function argument parser
    """
    argparser_baseline = subparsers.add_parser("baseline", help="Run baseline model")

    # group for input files
    group_input = argparser_baseline.add_argument_group("Input files and folders")
    group_input.add_argument('--data_dir', help='Data directory of kmer hdf5 files')

    # group for parameters
    group_params = argparser_baseline.add_argument_group("Training hyperparameters")
    group_params.add_argument('--batch_size', default=128, type=int, help='batch size')

    # group for output files
    group_output = argparser_baseline.add_argument_group("Output files and folders")
    add_out_dir_option(group_output, default='log_rf')
    group_output.add_argument('--prefix', default='tronn', help='prefix to attach onto file names')

    return


def track_runs(args):
    """track command and github commit
    """
    # keeps track of restores (or different commands) in folder
    num_restores = len(glob.glob('{0}/{1}.command'.format(args.out_dir, args.subcommand_name)))
    logging_file = '{0}/{1}.command_{2}.log'.format(args.out_dir, args.subcommand_name, num_restores)
    
    # track github commit
    git_repo_path = os.path.dirname(os.path.realpath(__file__))
    os.system('echo "commit:" > {0}'.format(logging_file))
    os.system('git -C {0} rev-parse HEAD >> {1}'.format(git_repo_path, logging_file))
    os.system('echo "" >> {0}'.format(logging_file))
    
    # write out the command
    with open(logging_file, 'a') as f:
        f.write(' '.join(sys.argv)+'\n\n')
    
    return logging_file


def main():
    """Main function for running TRoNN functions
    """
    args = parse_args()
    print "out_dir: %s" % args.out_dir
    os.system("mkdir -p {}".format(args.out_dir))
    logging_file = track_runs(args)
    #logging.basicConfig(filename=logging_file, level=logging.INFO)

    # get subcommand run function and run
    subcommand  = args.subcommand_name

    if subcommand == "preprocess":
        from tronn.preprocess import run
        run(args)
    elif subcommand == 'train':
        add_model_config(args)
        print 'model args: %s' % args.model
        from tronn.run_train import run
        run(args)
    elif subcommand == "extractimportances":
        add_model_config(args)
        print 'model args: %s' % args.model
        from tronn.run_extractimportances import run
        run(args)




    elif subcommand == 'interpret':
        add_model_config(args)
        print 'model args: %s' % args.model
        from tronn.interpret import run
        run(args)
    elif subcommand == 'interpret2':
        add_model_config(args)
        print 'model args: %s' % args.model
        from tronn.interpretation.wkm import run
        run(args)
    elif subcommand == 'scangrammars':
        from tronn.interpretation.grammars import run
        run(args)
    elif subcommand == "baseline":
        from tronn.baselines import run
        run(args)
        
    return None


if __name__ == '__main__':
    main()
