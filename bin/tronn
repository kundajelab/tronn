#!/usr/bin/env python

"""Description: TRONN main executable
"""

import os
import sys
import json
import glob
import logging
import argparse
import pkg_resources

from tronn.util.pwms import MotifSetManager # TODO move this?


def parse_args():
    """Prepare argument parser. Main subcommands are set up here first
    """
    parser = argparse.ArgumentParser(
        description='TRONN: Transcriptional Regulation Optimized Neural Nets')
    subparsers = parser.add_subparsers(dest='subcommand_name')

    # command for preprocessing data
    add_preprocess_parser(subparsers)
    
    # command for training
    add_train_parser(subparsers)

    # command for evaluation
    add_evaluate_parser(subparsers)

    # command for prediction
    add_predict_parser(subparsers)
    
    # command for motif scan
    add_scanmotifs_parser(subparsers)

    # command for dmim scan
    add_dmim_parser(subparsers)
    
    # command for network analysis
    # NOTE: this applies to stuff coming from scanmotifs and dmim
    add_buildgrammars_parser(subparsers)

    # command for synergy scan
    add_synergy_parser(subparsers)
    
    # command for activation maximization
    add_dream_parser(subparsers)

    # command for rational design with sig motifs
    # add_design_parser(subparsers)
    
    # TODO command for variant predictions
    add_analyzevariants_parser(subparsers)
    
    # baseline model
    add_baseline_parser(subparsers)
    
    # command for extracting model variables
    add_export_model_parser(subparsers)
    
    # parse args
    args = parser.parse_args()

    return args


def _add_output_args(parser, out_dir="./"):
    """Add output directory and prefix args to parser
    """
    parser.add_argument(
        "-o", "--out_dir", dest="out_dir", type=str,
        default=out_dir,
        help = "Output directory (default: current)")
    parser.add_argument(
        '--prefix', required=True,
        help='prefix to attach onto file names')

    return None


def _setup_annotations(args):
    """load annotation files from json and replace the json
    file with the dictionary
    """
    with open(args.annotations, "r") as fp:
        annotations = json.load(fp)
    args.annotations = annotations
    
    return None


def _parse_files(dataset_key_strings):
    """given an arg string list, parse out into a dict
    assumes a format of: key=file1,file2,..::param1=val,param2=val,...
    """
    if dataset_key_strings is None:
        return {}

    key_dict = {}
    for dataset_key_string in dataset_key_strings:
        # first split on "::" then on "="
        key_and_params = dataset_key_string.split("::")
        
        # set up key and key items (filenames or indices)
        key_string = key_and_params[0]
        key_and_items = key_string.split("=")
        key = key_and_items[0]
        if len(key_and_items) == 1:
            key_items = []
        else:
            item_string = key_and_items[1]
            key_items = [
                int(val) if _is_integer(val) else val
                for val in item_string.split(",")]
            
        # set up params
        if len(key_and_params) == 1:
            params = {}
        else:
            param_string = key_and_params[1]
            params = dict(
                [param.split("=")
                 for param in param_string.split(",")])
            
        # combine
        key_dict[key] = (key_items, params)
    
    return key_dict


def _setup_preprocess(args):
    """load preprocess files
    """
    args.labels = _parse_files(args.labels)
    args.signals = _parse_files(args.signals)

    return None


def add_preprocess_parser(subparsers):
    """Add data generation function argument parser
    """
    argparser_preprocess = subparsers.add_parser(
        "preprocess",
        help="Preprocess data into TRONN formats")

    # group for input files
    group_input = argparser_preprocess.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        "--annotations", required=True,
        help="json file of annotation files")
    group_input.add_argument(
        "--labels", nargs='+', required=True,
        help='list of key=files_separated_by_commas::params, files are in BED/narrowPeak format')
    group_input.add_argument(
        "--signals", nargs='+',
        help='list of key=files_separated_by_comma::params, files are in bigwig format')
    group_input.add_argument(
        "--master_label_keys", nargs="+", default=[],
        help="subset of label files to use to make master regions file")
    group_input.add_argument(
        "--master_bed_file", default=None,
        help="master regions file for all examples")

    # group for options
    group_opts = argparser_preprocess.add_argument_group(
        "Data generation options")
    group_opts.add_argument(
        "--rc", action="store_true",
        help='Reverse complement')
    group_opts.add_argument(
        "--genomewide", action="store_true",
        help='build examples across whole genome')
    group_opts.add_argument(
        "--parallel", default=12, type=int,
        help='Number of parallel threads to use')
    
    # group for output files
    group_output = argparser_preprocess.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="./datasets/dataset")
    group_output.add_argument(
        "--tmp_dir", help="directory for tmp files")

    return


def _add_dataset_args(parser):
    """add dataset args
    """
    parser.add_argument(
        "--data_format", default="hdf5",
        help="dataset storage format (hdf5, bed, custom)")
    parser.add_argument(
        "--data_dir",
        help="h5 file directory")
    parser.add_argument(
        "--data_files", nargs="+", default=[],
        help="dataset files")
    parser.add_argument(
        "--fasta",
        help="fasta file")
    parser.add_argument(
        "--dataset_json",
        help="use a json as input instead. other data args will supersede this json if requested")
    parser.add_argument(
        "--targets", nargs="+", default=[],
        help="which datasets to load as targets (ordered). Expected format: data_key=indices_list_with_commas")
    parser.add_argument(
        "--filter_targets", nargs="+", default=[],
        help="at least 1 of these targets must be POSITIVE. Format: data_key=indices_list_with_commas")
    parser.add_argument(
        "--singleton_filter_targets", nargs="+", default=[],
        help="2 or more of these targets must be POSITIVE. Format: data_key=indices_list_with_commas")
    parser.add_argument(
        "--target_indices", nargs="+", default=[], type=int,
        help="after collection of labels, which of those to finally select. Format: indices_list_with_commas")

    return None


def _parse_to_key_and_indices(key_strings):
    """given an arg string list, parse out into a dict
    assumes a format of: key=indices
    
    NOTE: this is for smart indexing into tensor dicts (mostly label sets)

    Returns:
      list of tuples, where tuple is (dataset_key, indices_list)
    """
    ordered_keys = []
    for key_string in key_strings:
        key_and_indices = key_string.split("=")
        key = key_and_indices[0]
        if len(key_and_indices) > 1:
            indices = [int(i) for i in key_and_indices[1].split(",")]
        else:
            indices = []
        ordered_keys.append((key, indices))
    
    return ordered_keys


def _setup_dataset_args(args):
    """set up dataset options
    """
    args.targets = _parse_to_key_and_indices(
        args.targets)
    args.filter_targets = _parse_to_key_and_indices(
        args.filter_targets)
    args.singleton_filter_targets = _parse_to_key_and_indices(
        args.singleton_filter_targets)

    if args.dataset_json is not None:
        with open(args.dataset_json, "r") as fp:
            args.dataset_json = json.load(fp)
    else:
        args.dataset_json = {}

    # and adjust as needed
    args.targets = args.dataset_json.get(
        "targets", args.targets)
    args.filter_targets = args.dataset_json.get(
        "filter_targets", args.filter_targets)
    args.singleton_filter_targets = args.dataset_json.get(
        "singleton_filter_targets", args.singleton_filter_targets)
    
    return None


def _add_model_args(parser):
    """add model args
    """
    parser.add_argument(
        "--model_framework", default="tensorflow",
        help="deep learning framework (tensorflow(tronn), keras, pytorch)")
    parser.add_argument(
        "--model", nargs="+",
        help="net function")
    parser.add_argument(
        "--model_json",
        help="use json as input instead")
    parser.add_argument(
        "--model_dir",
        help="model directory (if you want to update path)")
    parser.add_argument(
        "--model_checkpoint",
        help="model checkpoint (if you want to update checkpoint)")
    parser.add_argument(
        "--logit_indices", nargs="+", default=[], type=int,
        help="which logits from net to load")

    return None


def _setup_model_args(args):
    """set up model options that came in through cmd line
    assumes a format of: model_name param=val,param=val,...

    Returns:
      converts args.model to dict of all model inputs
    """
    # first set up everything through args.model
    if args.model is not None:
        model = {
            "name": args.model[0],
            "params": {}}
        for model_arg in args.model[1:]:
            if '=' in model_arg:
                # params
                key, value = model_arg.split('=', 1)
                # check if param is a list
                if "," in value:
                    model["params"][key] = value.split(",")
                else:
                    model["params"][key] = eval(value)
            else:
                # action store true
                model["params"][model_arg] = True
        args.model = model
    else:
        with open(args.model_json, "r") as fp:
            args.model = json.load(fp)

    # update checkpoint if needed
    if args.model_checkpoint is not None:
        args.model["checkpoint"] = args.model_checkpoint
        
    # update model dir (and checkpoint location) if needed
    if args.model_dir is not None:
        args.model["model_dir"] = args.model_dir
        if args.model.get("checkpoint") is not None:
            args.model["checkpoint"] = "{}/{}".format(
                args.model_dir,
                os.path.basename(args.model["checkpoint"]))

    # if model dir is still none, use out_dir
    if args.model.get("model_dir") is None:
        args.model["model_dir"] = args.out_dir
    
    return None


def add_train_parser(subparsers):
    """Add argument parser for training
    """
    cmd_argparser = subparsers.add_parser(
        "train",
        help="Train a NN model")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    
    # group for cross validation params
    group_cv = cmd_argparser.add_argument_group(
        "Cross validation params")
    group_cv.add_argument(
        "--full_train", action="store_true",
        help="ignore early stopping, train for full num epochs")
    group_cv.add_argument(
        "--kfolds", default=10, type=int,
        help="number of folds to split the data")
    group_cv.add_argument(
        "--valid_folds", nargs="+", default=[0], type=int,
        help="which fold to use as validation")
    group_cv.add_argument(
        "--test_folds", nargs="+", default=[1], type=int,
        help="which fold to use as test")

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    group_model.add_argument(
        '--transfer_model_checkpoint', default=None,
        help='transfer from desired checkpoint') 
    group_model.add_argument(
        '--restore_model_checkpoint', default=None,
        help='restore from desired checkpoint (in new dir)') 
    
    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "Training arguments")
    group_params.add_argument(
        '--epochs', default=30, type=int,
        help='number of epochs')
    group_params.add_argument(
        '--batch_size', default=256, type=int,
        help='batch size')
    group_params.add_argument(
        '--early_stopping_metric', default='mean_auprc', type=str,
        help='metric to use for early stopping')
    group_params.add_argument(
        '--patience', default=3, type=int,
        help='metric to use for early stopping')
    group_params.add_argument(
        "--regression", action="store_true",
        help="run regression instead of classification")
    group_params.add_argument(
        "--distributed", action="store_true",
        help="run on multi gpus")
    group_params.add_argument(
        '--finetune_targets', nargs="+", default=[],
        help="tasks to finetune")

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='model')
    group_output.add_argument(
        "--get_dataset_metrics", action="store_true",
        help="calculate dataset metrics if needed")

    return


def add_evaluate_parser(subparsers):
    """Add argument parser for test-time evaluation
    """
    cmd_argparser = subparsers.add_parser(
        "evaluate",
        help="Evaluate model")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    
    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "Parameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
    	"--regression", action="store_true",
    	help="evaluate regression")
    group_params.add_argument(
    	"--num_evals", default=64000, type=int,
    	help="Number of steps to run for prediction")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='evaluation')
    
    return


def add_predict_parser(subparsers):
    """Add argument parser for predicting with trained models
    """
    cmd_argparser = subparsers.add_parser(
        "predict",
        help="Predict with trained model")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)

    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "Parameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
    	"--regression", action="store_true",
    	help="evaluate regression")
    group_params.add_argument(
    	"--num_evals", default=1000, type=int,
    	help="Number of steps to run for prediction")

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='prediction')
    
    return


def _add_interpretation_args(parser, inference_fn="sequence_to_motif_scores"):
    """set up trained model info
    """
    parser.add_argument(
        "--inference_targets", nargs="+", required=True, type=int,
        help="tasks to extract importance scores - must be global indices") 
    parser.add_argument(
        "--backprop", default="input_x_grad",
        help="what method to use for interpretation")
    parser.add_argument(
        "--inference_fn", default=inference_fn,
        help="inference stack")
    parser.add_argument(
        "--pwm_file", default=None,
        help="pwm file")
    parser.add_argument(
        "--sample_size", default=100000, type=int,
        help="total regions (post filtering) to return")
    parser.add_argument(
        "--debug", action="store_true",
        help="visualize outputs to help debug. NOTE: use small sample size!")
    
    return


def _setup_interpretation_args(args):
    """given parsed args, adjust inputs as needed
    matches _add_interpretation_options
    """
    # pwm file
    args.pwm_list = MotifSetManager.read_pwm_file(args.pwm_file)
    args.pwm_names = [pwm.name for pwm in args.pwm_list]
    args.pwm_dict = MotifSetManager.read_pwm_file(args.pwm_file, as_dict=True)

    # set up inference dict
    args.inference_params = {
        "cmd_name": args.subcommand_name,
        "inference_mode": True, # for estimator framework
        "model_reuse": True if args.model_framework == "tensorflow" else False,
        "inference_fn_name": args.inference_fn,
        "importance_task_indices": args.inference_targets,
        "backprop": args.backprop,
        "pwms": args.pwm_list}
    
    return


def _setup_scanmotifs_args(args):
    """given parsed args, adjust inputs as needed
    """
    if args.foreground_targets is not None:
        args.foreground_targets = _parse_to_key_and_indices(
            args.foreground_targets)

    return


def _setup_dmim_args(args):
    """given parsed args, adjust inputs as needed
    """
    args.foreground_targets = _parse_to_key_and_indices(
        args.foreground_targets)

    return


def _add_sig_pwms_args(parser):
    """add in args to use sig pwm vectors
    """
    parser.add_argument(
        "--sig_pwms_file",
        help="if this arg is used, pvals will be used from here and not from scan file")
    parser.add_argument(
        "--sig_pwms_key", default="pwms.differential",
        help="which dataset group has the sig pwm vectors")

    return


def _add_visualization_args(parser):
    """set up visualization after interpretation options
    """
    parser.add_argument(
        "--visualize_R", nargs="+", default=[],
        help="visualize with R scripts")
    parser.add_argument(
        "--visualize_multikey_R", nargs="+", default=[],
        help="visualize with R scripts")
    
    return


def _setup_visualization_args(args):
    """given parsed args, adjust visualization inputs as needed
    matches _add_visualization_options
    """
    if args.visualize_R is not None:
        args.visualize_R = _parse_files(args.visualize_R)
    if args.visualize_multikey_R is not None:
        args.visualize_multikey_R = _parse_files(args.visualize_multikey_R)

    return


def add_scanmotifs_parser(subparsers):
    """Scan motifs
    """
    cmd_argparser = subparsers.add_parser(
        "scanmotifs",
        help="Scan sequence with motifs from a PWM file")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    # placeholder right now (to make downstream processing in interp equal)
    # figure out how to remove?
    group_dataset.add_argument( 
        "--processed_inputs", action="store_true",
        help="inputs are already preprocessed through some inference stack")

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=48, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_motif_scores") # adjust for defaults
    _add_visualization_args(group_interpret)

    # group for cmd specific functions
    group_other = cmd_argparser.add_argument_group(
        "Other options")
    
    group_other.add_argument(
        "--foreground_targets", nargs="+",
        help="which targets to use as foreground for calling motif enrichment")
    group_other.add_argument(
        "--background_scores",
        help="h5 file of background scores")

    
    group_other.add_argument(
        "--background_targets", nargs="+",
        help="which targets to use as background for calling motif enrichment. MUST MATCH FOREGROUND ORDER EXACTLY")
    group_other.add_argument(
        "--background_filter_targets", nargs="+", default=[],
        help="targets to filter for background set of regions") # these need to match exactly?
    group_other.add_argument(
        "--background_sample_multiplier", default=2,
        help="Multiplicative factor for num examples in background to collect (ie, 2X the number of foreground)")
    

    group_other.add_argument(
        "--cluster", action="store_true",
        help="run clustering")
    group_other.add_argument(
        "--summarize_manifold", action="store_true",
        help="summarize clusters in the manifold")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="motifs")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory as needed')

    return


def add_buildgrammars_parser(subparsers):
    """build grammars from motif scores or mutational results
    """
    cmd_argparser = subparsers.add_parser(
        "buildgrammars",
        help="build grammars from scanning results (motif or mutational)")

    # group for type
    group_analysis = cmd_argparser.add_argument_group(
        "Grammar build type")
    group_analysis.add_argument(
        "--scan_type", default="dmim",
        help="type of results being used")
    group_analysis.add_argument(
        "--min_positive_tasks", default=2, type=int,
        help="minimum cell states (tasks) in which effect exists to generate edge")
    group_analysis.add_argument(
        "--subgraph_max_k", default=3, type=int,
        help="when searching graph, max nodes to include in a subgraph")
    group_analysis.add_argument(
        "--min_support", default=500, type=int,
        help="min number of examples needed to support a grammar")
    group_analysis.add_argument(
        "--max_overlap_fraction", default=0.3, type=float,
        help="max fraction of regions that can overlap between two subgraphs")
    group_analysis.add_argument(
        "--return_top_k", type=int,
        help="if requested return just the top k subgraphs, ranked by output strength")

    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    group_dataset.add_argument(
        "--scan_file", required=True,
        help="h5 file of scan results")
    #group_dataset.add_argument(
    #    "--scan_key",
    #    help="dataset key with scores") # either motifs {N, task, M} or dmim {N, mutM, task, M}, with dmim need to consider outputs logits {N, mutM, logits}
    _add_sig_pwms_args(group_dataset)
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="grammars")
    
    return


def add_dmim_parser(subparsers):
    """scan grammars from a grammar file
    """
    cmd_argparser = subparsers.add_parser(
        "dmim",
        help="score sequences for grammars")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    group_dataset.add_argument(
        "--processed_inputs", action="store_true",
        help="inputs are already preprocessed through some inference stack")
    _add_sig_pwms_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
        
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=8, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_dmim")
    _add_visualization_args(group_interpret)

    # group for cmd specific functions
    group_other = cmd_argparser.add_argument_group(
        "Misc params")
    group_other.add_argument(
        "--manifold_file", help="manifold h5 file")
    group_other.add_argument(
        "--foreground_targets", nargs="+",
        help="which targets to use as foreground for calling motif enrichment")

    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="dmim")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory')
    
    return


def add_synergy_parser(subparsers):
    """scan grammars from a grammar file
    """
    cmd_argparser = subparsers.add_parser(
        "synergy",
        help="score sequences for grammars")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    group_dataset.add_argument(
        "--processed_inputs", action="store_true",
        help="inputs are already preprocessed through some inference stack")

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
        
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=8, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_synergy")
    _add_visualization_args(group_interpret)

    # group for cmd specific functions
    group_other = cmd_argparser.add_argument_group(
        "Misc params")
    group_other.add_argument(
        "--grammar_file", help="grammar (gml) file")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="dmim")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory')
    
    return


def add_dream_parser(subparsers):
    """activation maximization
    """
    cmd_argparser = subparsers.add_parser(
        "dream",
        help="use activation maximization to generate novel synthetic sequences")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset) # use BED input? or array input

    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    _add_interpretation_args(group_interpret)

    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "dream hyperparameters")
    group_params.add_argument(
        "--max_iter", default=20, type=int,
        help="number of iterations for dreaming")
    group_params.add_argument(
        "--edit_dist", default=1000, type=int,
        help="edit distance from start position allowed")

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="dream")
    group_output.add_argument(
        "--plot", action="store_true",
        help="show development of sequence across time")

    return 


def add_analyzevariants_parser(subparsers):
    """analyze variants
    """
    cmd_argparser = subparsers.add_parser(
        "analyzevariants",
        help="Scan sequence with motifs from a PWM file")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    group_dataset.add_argument(
        "--processed_inputs", action="store_true",
        help="inputs are already preprocessed through some inference stack")

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)

    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    _add_interpretation_args(group_interpret)

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="motifs")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory as needed')
    
    return


def add_vizimpts_parser(subparsers):
    """
    """
    argparser_multitask = subparsers.add_parser(
        "vizimportances",
        help="Get back importance score visualizations for a region")


    # group for input files
    # start with BED file, preprocess, then run through
    group_input = argparser_multitask.add_argument_group(
    	"Input files and folders")
    load_data_files(group_input)
    group_input.add_argument(
        "--labels", nargs='+', required=True,
        help='list of label file peak sets (BED/narrowPeak)')
    group_input.add_argument(
        "--importances_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")
    
    # group for model
    group_model = argparser_multitask.add_argument_group(
        "Model definition")
    #add_trained_nn_model_options(group_model)

    # group for parameters
    group_params = argparser_multitask.add_argument_group(
        "Interpretation hyperparameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')

    # group for output files
    group_output = argparser_multitask.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="importances")
    group_output.add_argument(
        '--tmp_dir', default='importances_tmp',
        help='temporary scratch directory')
    group_output.add_argument(
        '--plot_samples', action='store_true',
        help='plot some sample sequences weighted by importance scores')

    return



def add_baseline_parser(subparsers):
    """Add train baseline function argument parser
    """
    argparser_baseline = subparsers.add_parser("baseline", help="Run baseline model")

    # group for input files
    group_input = argparser_baseline.add_argument_group("Input files and folders")
    group_input.add_argument('--data_dir', help='Data directory of kmer hdf5 files')
    group_input.add_argument(
        "--cvfold", default=0, type=int,
        help="which files to train on [0, 1, 2]")
    group_input.add_argument(
        "--num_classes", default=2, type=int,
        help="how many classes")
    group_input.add_argument(
        "--kmers", action="store_true",
        help="featurize as kmers")
    group_input.add_argument(
        "--kmer_len", default=6, type=int,
        help="what kmer len to featurize with")
    group_input.add_argument(
        "--motifs", action="store_true",
        help="featurize as kmers")
    group_input.add_argument(
        "--pwm_file",
        help="motif_file")
    group_input.add_argument(
        "--tasks", nargs="+", default=[], type=int,
        help="tasks to train on (default is all)")

    # group for parameters
    group_params = argparser_baseline.add_argument_group("Training hyperparameters")
    group_params.add_argument('--batch_size', default=128, type=int, help='batch size')
    group_params.add_argument('--num_trees', default=100, type=int, help='how many trees')
    group_params.add_argument('--max_nodes', default=10000, type=int, help='max nodes per tree')
    group_params.add_argument('--num_evals', default=1000, type=int, help='num evaluation batches')

    # group for output files
    group_output = argparser_baseline.add_argument_group("Output files and folders")
    _add_output_args(group_output, out_dir='baseline')

    return


def add_export_model_parser(subparsers):
    """Add argument parser for extracting trained params from a model
    """
    cmd_argparser = subparsers.add_parser(
        "exportmodel",
        help="extract model variables")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    group_model.add_argument(
        "--skip", nargs="+", default=[],
        help="substrings in variable names to skip")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='export')
    
    return


def track_runs(args):
    """track command and github commit
    """
    # keeps track of restores (or different commands) in folder
    num_restores = len(glob.glob('{0}/{1}.command*'.format(args.out_dir, args.subcommand_name)))
    logging_file = '{0}/{1}.command_{2}.log'.format(args.out_dir, args.subcommand_name, num_restores)
    
    # track github commit
    git_repo_path = os.path.dirname(os.path.realpath(__file__))
    os.system('echo "commit:" > {0}'.format(logging_file))
    os.system('git --git-dir={0}/.git rev-parse HEAD >> {1}'.format(
        git_repo_path.split("/bin")[0], logging_file))
    os.system('echo "" >> {0}'.format(logging_file))
    
    # write out the command
    with open(logging_file, 'a') as f:
        f.write(' '.join(sys.argv)+'\n\n')
    
    return logging_file


def _setup_logs(args):
    """set up logging
    """
    logging_file = track_runs(args)
    reload(logging)
    logging.basicConfig(
        filename=logging_file,
        level=logging.DEBUG, # TODO ADJUST BEFORE RELEASE
        format='%(message)s')
    logging.getLogger().addHandler(logging.StreamHandler())
    for arg in sorted(vars(args)):
        logging.info("{}: {}".format(arg, getattr(args, arg)))
    logging.info("")

    return


def _is_integer(val):
    """helper function for parsing
    """
    try:
        int(val)
        return True
    except ValueError:
        return False
    
    

def main():
    """Main function for running TRoNN functions
    """
    # parse args and set up
    args = parse_args()
    args.out_dir = os.path.abspath(args.out_dir)
    os.system("mkdir -p {}".format(args.out_dir))
    _setup_logs(args)

    # get subcommand run function and run
    subcommand = args.subcommand_name

    if subcommand == "preprocess":
        _setup_annotations(args)
        _setup_preprocess(args)
        from tronn.preprocess_cmd import run
        run(args)
    elif subcommand == "preprocess_variants":
        _setup_annotations(args)
        _setup_preprocess(args)
        from tronn.preprocess_variants_cmd import run
        run(args)
    elif subcommand == 'train':
        _setup_dataset_args(args)
        _setup_model_args(args)
        from tronn.train_cmd import run
        run(args)
    elif subcommand == "evaluate":
        _setup_dataset_args(args)
        _setup_model_args(args)
        from tronn.evaluate_cmd import run
        run(args)
    elif subcommand == "predict":
        _setup_dataset_args(args)
        _setup_model_args(args)
        from tronn.predict_cmd import run
        run(args)
    elif subcommand == "scanmotifs":
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_interpretation_args(args)
        _setup_visualization_args(args)
        _setup_scanmotifs_args(args)
        from tronn.scanmotifs_cmd import run
        run(args)
    elif subcommand == "dmim":
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_interpretation_args(args)
        _setup_visualization_args(args)
        _setup_dmim_args(args)
        from tronn.dmim_cmd import run
        run(args)
    elif subcommand == "buildgrammars":
        from tronn.buildgrammars_cmd import run
        run(args)
    elif subcommand == "synergy":
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_interpretation_args(args)
        _setup_visualization_args(args)
        from tronn.synergy_cmd import run
        run(args)
    elif subcommand == "dream":
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_interpretation_options(args)
        from tronn.dream_cmd import run
        run(args)
    elif subcommand == "analyzevariants":
        _setup_dataset_args(args)
        _setup_model(args)
        from tronn.analyzevariants_cmd import run
        run(args)
    elif subcommand == "baseline":
        _setup_dataset_args(args)
        from tronn.run_tensorforest import run
        run(args)
    elif subcommand == "exportmodel":
        _setup_dataset_args(args)
        _setup_model_args(args)
        from tronn.export_cmd import run
        run(args)
    # add new commands here
        
    return None


if __name__ == '__main__':
    main()
