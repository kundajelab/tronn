#!/usr/bin/env python

"""Description: TRONN main executable
"""

import os
import sys
import h5py
import json
import glob
import logging
import argparse
import pkg_resources

from tronn.interpretation.pwms import read_pwm_file


def parse_args():
    """Prepare argument parser. Main subcommands are set up here first
    """
    parser = argparse.ArgumentParser(
        description='TRONN: Transcriptional Regulation Optimized Neural Nets')
    subparsers = parser.add_subparsers(dest='subcommand_name')

    # command for preprocessing data
    add_preprocess_parser(subparsers)

    # command for preprocessing variants
    add_preprocess_variants_parser(subparsers)
    
    # command for training
    add_train_parser(subparsers)

    # command for evaluation
    add_evaluate_parser(subparsers)

    # command for prediction
    add_predict_parser(subparsers)
    
    # command for motif scan
    add_scanmotifs_parser(subparsers)

    # command for dmim scan
    add_dmim_parser(subparsers)
    
    # command for activation maximization
    add_dream_parser(subparsers)

    # command for rational design with sig motifs
    # add_design_parser(subparsers)
    
    # TODO command for variant predictions
    add_analyzevariants_parser(subparsers)
    
    # TODO:    
    # modisco?
    # add_modisco_parser(subparsers)
    
    # baseline model
    add_baseline_parser(subparsers)
    
    # TODO call this model export
    add_extract_params_parser(subparsers)
    
    # parse args
    args = parser.parse_args()

    return args


# TODO(dk) fix this to add more options to model param setup
def add_model_params(args):
    """Add model configs
    """
    # parse model configs
    model_config = {}
    model_config['name'] = args.model[0]
    for model_arg in args.model[1:]:
        if "models" in model_arg:
            name, models = model_arg.split("=", 1)
            model_config[name] = models
        elif '=' in model_arg:
            name, value = model_arg.split('=', 1)
            model_config[name] = eval(value)
        else:
            model_config[model_arg] = True
    args.model = model_config

    return 


def add_output_options(parser, out_dir="./"):
    """Add an output directory and prefix if desired
    """
    parser.add_argument(
        "-o", "--out_dir", dest="out_dir", type=str,
        default=out_dir,
        help = "Output directory (default: current)")
    parser.add_argument(
        '--prefix', required=True,
        help='prefix to attach onto file names')

    return


def add_preprocess_parser(subparsers):
    """Add data generation function argument parser
    """
    argparser_preprocess = subparsers.add_parser(
        "preprocess",
        help="Preprocess data into TRONN formats")

    # group for input files
    group_input = argparser_preprocess.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        "--annotations", required=True,
        help="json file of annotation files")
    group_input.add_argument(
        "--labels", nargs='+', required=True,
        help='list of key=files_separated_by_commas;params, files are in BED/narrowPeak format')
    group_input.add_argument(
        "--signals", nargs='+',
        help='list of key=files_separated_by_commas;params, files are in bigwig format')
    group_input.add_argument(
        "--master_label_keys", nargs="+", default=[],
        help="subset of label files to use to make master regions file")
    group_input.add_argument(
        "--master_bed_file", default=None,
        help="master regions file for all examples")

    # group for options
    group_opts = argparser_preprocess.add_argument_group(
        "Data generation options")
    group_opts.add_argument(
        "--rc", action="store_true",
        help='Reverse complement')
    #group_opts.add_argument(
    #    "--no_flank_negs", action="store_true",
    #    help="turn off flank negatives")
    #group_opts.add_argument(
    #    "--no_dhs_negs", action="store_true",
    #    help="turn off flank negatives")
    #group_opts.add_argument(
    #    "--random_negs", action="store_true",
    #    help="add random negatives")
    #group_opts.add_argument(
    #    "--univ_neg_num", type=int,
    #    help='number of universal negatives to grab from univ DHS regions')
    group_opts.add_argument(
        "--genomewide", action="store_true",
        help='build examples across whole genome')
    group_opts.add_argument(
        "--parallel", default=12, type=int,
        help='Number of parallel threads to use')
    #group_opts.add_argument(
    #    "--kmerize", action='store_true',
    #    help="generate kmer datasets also")
    
    # group for output files
    group_output = argparser_preprocess.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="dataset")
    group_output.add_argument(
        "--tmp_dir", help="directory for tmp files")

    return


def add_preprocess_variants_parser(subparsers):
    """Add data generation function argument parser
    """
    argparser_preprocess = subparsers.add_parser(
        "preprocess_variants",
        help="Preprocess data into TRONN formats")

    # group for input files
    group_input = argparser_preprocess.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        "--annotations", required=True,
        help="json file of annotation files")
    group_input.add_argument(
        "--labels", nargs='+', required=True,
        help='list of key=files_separated_by_commas;params, files are in BED/narrowPeak format')
    group_input.add_argument(
        "--signals", nargs='+',
        help='list of key=files_separated_by_commas;params, files are in bigwig format')
    group_input.add_argument(
        "--master_label_keys", nargs="+", default=[],
        help="subset of label files to use to make master regions file")
    group_input.add_argument(
        "--vcf_file",
        help="vcf file with ref/alt")

    # group for options
    group_opts = argparser_preprocess.add_argument_group(
        "Data generation options")
    group_opts.add_argument(
        "--rc", action="store_true",
        help='Reverse complement')
    group_opts.add_argument(
        "--parallel", default=12, type=int,
        help='Number of parallel threads to use')
    
    # group for output files
    group_output = argparser_preprocess.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="dataset")
    group_output.add_argument(
        "--tmp_dir", help="directory for tmp files")

    return


# TODO add a cvfold selection method


def add_train_parser(subparsers):
    """Add argument parser for training
    """
    argparser_train = subparsers.add_parser(
        "train",
        help="Train a TRONN model")
    
    # group for input files
    group_input = argparser_train.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        "--data_dir", required=True,
        help="h5 file directory")
    group_input.add_argument(
        "--label_keys", nargs="+", default=[],
        help="which datasets to load as labels")
    group_input.add_argument(
        "--tasks", nargs="+", default=[], type=int,
        help="tasks to train on (default is all)")
    #group_input.add_argument(
    #    "--cvfold", default=0, type=int,
    #    help="which files to train on [0, 1, 2]") # TODO adjust this
    group_input.add_argument(
        '--restore_model_dir', default=None,
        help='restore from last checkpoint in dir')
    group_input.add_argument(
        '--restore_model_checkpoint', default=None,
        help='restore from desired checkpoint')
    group_input.add_argument(
        '--transfer_model_dir', default=None,
        help='transfer from last checkpoint in dir')
    group_input.add_argument(
        '--transfer_model_checkpoint', default=None,
        help='transfer from desired checkpoint') 
    
    # group for model
    group_model = argparser_train.add_argument_group(
        "Model definition")
    group_model.add_argument(
        '--model', nargs='+', required=True,
        help='choose model and provide configs')
    
    # group for parameters
    group_params = argparser_train.add_argument_group(
        "Training hyperparameters")
    group_params.add_argument(
        '--epochs', default=20, type=int,
        help='number of epochs')
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
        '--metric', default='mean_auprc', type=str,
        help='metric to use for early stopping')
    group_params.add_argument(
        '--patience', default=2, type=int,
        help='metric to use for early stopping')
    group_params.add_argument(
        '--finetune_tasks', nargs="+", default=[],
        help="tasks to finetune")

    # group for output files
    group_output = argparser_train.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir='model')

    return


# TODO deprecate in favor of json loading
def add_trained_nn_model_options(parser, required=True):
    """Set up standard necessary args for a trained NN model
    """
    parser.add_argument(
        '--model', nargs='+', required=required,
        help='choose model and provide configs')
    parser.add_argument(
        '--model_dir', default=None,
        help='restore from last checkpoint')
    parser.add_argument(
        "--model_checkpoints", nargs="+", default=[],
        help="restore from specific checkpoints (they are loaded in order)")
    
    return


def add_evaluate_parser(subparsers):
    """Add argument parser for test-time evaluation
    """
    argparser_test = subparsers.add_parser(
        "evaluate",
        help="Evaluate TRONN model")

    # group for input files
    group_input = argparser_test.add_argument_group(
        "Input files and folders")
    # TODO add an option for different eval set?
    # but better to force user to pre-define test set
    group_input.add_argument(
        "--model_info", required=True,
        help="json with training info")
    
    # group for parameters
    group_params = argparser_test.add_argument_group(
        "Parameters")
    group_params.add_argument(
    	"--num_evals", default=512000, type=int,
    	help="Number of steps to run for prediction")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    #group_params.add_argument(
    #    "--single_task", default=None, type=int,
    #    help="Evaluate ALL predictions on ONE task (give idx here)")
    
    # group for output files
    group_output = argparser_test.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir='evaluation')
    
    return


def add_predict_parser(subparsers):
    """Add argument parser for predicting with trained models
    """
    argparser_predict = subparsers.add_parser(
        "predict",
        help="Predict with trained model")

    # group for input files
    group_input = argparser_predict.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        "--data_dir", required=False,
        help="hdf5 file directory")
    group_input.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_input.add_argument(
        "--model_info", required=True,
        help="json with training info")

    group_input.add_argument(
        "--input_bed", default=None,
        help="give an input BED file for prediction")
    group_input.add_argument(
        "--input_labels", nargs='+', default=[],
        help='list of label file peak sets (BED/narrowPeak). MUST MATCH INITIAL LABEL SET')

    # group for parameters
    group_params = argparser_predict.add_argument_group(
        "Parameters")
    group_params.add_argument(
    	"--num_evals", default=1000, type=int,
    	help="Number of steps to run for prediction")
    #group_params.add_argument(
   # 	"--reconstruct_regions", action="store_true",
   # 	help="Produce files where values are merged")
    group_params.add_argument(
        "--single_task", default=None, type=int,
        help="Get ALL predictions on ONE task (give idx here)")

    # group for output files
    group_output = argparser_predict.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir='prediction')
    
    return


def _add_interpretation_inputs(parser):
    """set up interpretation inputs
    """
    parser.add_argument(
        "--data_dir", required=True,
        help="h5 data directory for inference")
    parser.add_argument(
        '--batch_size', default=64, type=int,
        help='batch size')
    parser.add_argument(
        "--model_info", required=True,
        help="json with training info")
    
    return


def _add_interpretation_options(parser):
    """set up trained model info
    """
    # REMEMBER important that inference tasks are strong predictor tasks
    # TODO: consider set up like --inference_tasks ATAC_labels=0,1,2,3,4,5,6,9,10,12 tf_labels
    # this requires downstream conversion into correct indices
    
    parser.add_argument(
        "--inference_tasks", nargs="+", required=True,
        help="tasks to extract importance scores")
    parser.add_argument(
        "--filter_tasks", nargs="+", default=[],
        help="tasks to filter on") # TODO make sure that filtering is OFF for dreaming
    parser.add_argument(
        "--backprop", default="input_x_grad",
        help="what method to use for interpretation")
    parser.add_argument(
        "--inference_fn",
        help="inference stack")
    parser.add_argument(
        "--pwm_file", default=None,
        help="pwm file")
    parser.add_argument(
        "--sample_size", default=100000, type=int,
        help="total regions (post filtering) to return")
    parser.add_argument(
        "--debug", action="store_true",
        help="visualize outputs to help debug. NOTE: use small sample size!")

    
    return


def _setup_interpretation_options(args):
    """given parsed args, adjust inputs as needed
    matches _add_interpretation_options
    """
    # inference tasks
    args.inference_tasks = _parse_files(args.inference_tasks)
    args.inference_task_indices = _get_absolute_label_indices(
        args.model_info, args.inference_tasks)

    # filter tasks
    args.filter_tasks = _parse_files(args.filter_tasks)
    args.filter_task_indices = _get_absolute_label_indices(
        args.model_info, args.filter_tasks)

    # pwm file
    args.pwm_list = read_pwm_file(args.pwm_file)
    args.pwm_names = [pwm.name for pwm in args.pwm_list]
    args.pwm_dict = read_pwm_file(args.pwm_file, as_dict=True)

    return


def _add_visualization_options(parser):
    """set up visualization after interpretation options
    """
    parser.add_argument(
        "--visualize_tasks", nargs="+", default=[],
        help="label files")
    parser.add_argument(
        "--visualize_signals", nargs="+", default=[],
        help="signal files with corresponding label sets")
    
    return


def _setup_visualization_options(args):
    """given parsed args, adjust visualization inputs as needed
    matches _add_visualization_options
    """
    # visualize tasks
    if args.visualize_tasks is not None:
        args.visualize_tasks = _parse_files(args.visualize_tasks)
        args.visualize_task_indices = _get_absolute_label_indices(
            args.model_info, args.visualize_tasks, list_of_lists=True)

    # visualize signals
    if args.visualize_signals is not None:
        args.visualize_signals = _parse_files(args.visualize_signals)

    return


def add_scanmotifs_parser(subparsers):
    """Scan motifs
    """
    cmd_argparser = subparsers.add_parser(
        "scanmotifs",
        help="Scan sequence with motifs from a PWM file")
    
    # group for input, input filtering, and model
    group_input = cmd_argparser.add_argument_group(
    	"Input files and folders")
    _add_interpretation_inputs(group_input)
    
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    _add_interpretation_options(group_interpret)
    _add_visualization_options(group_interpret)
    

    # group for cmd specific functions
    group_other = cmd_argparser.add_argument_group(
        "Misc params")
    group_other.add_argument(
        "--cluster", action="store_true",
        help="run clustering")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="motifs")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory as needed')
    #group_output.add_argument(
    #    '--plot_importance_sample', action='store_true',
    #    help='plot some sample importance score weighted samples')

    return


def add_dmim_parser(subparsers):
    """scan grammars from a grammar file
    """
    cmd_argparser = subparsers.add_parser(
        "dmim",
        help="score sequences for grammars")
    
    # group for input files
    group_input = cmd_argparser.add_argument_group(
    	"Input files and folders")
    _add_interpretation_inputs(group_input)
        
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    _add_interpretation_options(group_interpret)
    _add_visualization_options(group_interpret)

    # group for cmd specific functions
    group_other = cmd_argparser.add_argument_group(
        "Misc params")
    group_other.add_argument(
        "--manifold_file", help="manifold h5 file")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="dmim")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory')
    
    return


def add_dream_parser(subparsers):
    """activation maximization
    """
    cmd_argparser = subparsers.add_parser(
        "dream",
        help="use activation maximization to generate novel synthetic sequences")

    # inputs
    group_input = cmd_argparser.add_argument_group(
    	"Input files and folders")
    group_input.add_argument(
        "--sequence_file",
        help="sequence file of starting points with activation patterns")
    group_input.add_argument(
        "--model_info", required=True,
        help="json with training info")

    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    _add_interpretation_options(group_interpret)

    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "dream hyperparameters")
    group_params.add_argument(
        "--max_iter", default=20, type=int,
        help="number of iterations for dreaming")
    group_params.add_argument(
        "--edit_dist", default=1000, type=int,
        help="edit distance from start position allowed")

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="dream")
    group_output.add_argument(
        "--plot", action="store_true",
        help="show development of sequence across time")

    return 


def add_analyzevariants_parser(subparsers):
    """analyze variants
    """
    cmd_argparser = subparsers.add_parser(
        "analyzevariants",
        help="Scan sequence with motifs from a PWM file")
    
    # group for input files
    group_input = cmd_argparser.add_argument_group(
    	"Input files and folders")
    _add_interpretation_inputs(group_input)

    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    _add_interpretation_options(group_interpret)

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="motifs")
    group_output.add_argument(
        '--tmp_dir',
        help='temporary scratch directory as needed')
    
    return


def add_vizimpts_parser(subparsers):
    """
    """
    argparser_multitask = subparsers.add_parser(
        "vizimportances",
        help="Get back importance score visualizations for a region")


    # group for input files
    # start with BED file, preprocess, then run through
    group_input = argparser_multitask.add_argument_group(
    	"Input files and folders")
    load_data_files(group_input)
    group_input.add_argument(
        "--labels", nargs='+', required=True,
        help='list of label file peak sets (BED/narrowPeak)')
    group_input.add_argument(
        "--importances_tasks", nargs="+", required=True, type=int,
        help="tasks to interpret on")
    
    # group for model
    group_model = argparser_multitask.add_argument_group(
        "Model definition")
    add_trained_nn_model_options(group_model)

    # group for parameters
    group_params = argparser_multitask.add_argument_group(
        "Interpretation hyperparameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')

    # group for output files
    group_output = argparser_multitask.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir="importances")
    group_output.add_argument(
        '--tmp_dir', default='importances_tmp',
        help='temporary scratch directory')
    group_output.add_argument(
        '--plot_samples', action='store_true',
        help='plot some sample sequences weighted by importance scores')

    return



def add_baseline_parser(subparsers):
    """Add train baseline function argument parser
    """
    argparser_baseline = subparsers.add_parser("baseline", help="Run baseline model")

    # group for input files
    group_input = argparser_baseline.add_argument_group("Input files and folders")
    group_input.add_argument('--data_dir', help='Data directory of kmer hdf5 files')
    group_input.add_argument(
        "--cvfold", default=0, type=int,
        help="which files to train on [0, 1, 2]")
    group_input.add_argument(
        "--num_classes", default=2, type=int,
        help="how many classes")
    group_input.add_argument(
        "--kmers", action="store_true",
        help="featurize as kmers")
    group_input.add_argument(
        "--kmer_len", default=6, type=int,
        help="what kmer len to featurize with")
    group_input.add_argument(
        "--motifs", action="store_true",
        help="featurize as kmers")
    group_input.add_argument(
        "--pwm_file",
        help="motif_file")
    group_input.add_argument(
        "--tasks", nargs="+", default=[], type=int,
        help="tasks to train on (default is all)")

    # group for parameters
    group_params = argparser_baseline.add_argument_group("Training hyperparameters")
    group_params.add_argument('--batch_size', default=128, type=int, help='batch size')
    group_params.add_argument('--num_trees', default=100, type=int, help='how many trees')
    group_params.add_argument('--max_nodes', default=10000, type=int, help='max nodes per tree')
    group_params.add_argument('--num_evals', default=1000, type=int, help='num evaluation batches')

    # group for output files
    group_output = argparser_baseline.add_argument_group("Output files and folders")
    add_output_options(group_output, out_dir='baseline')

    return


def add_extract_params_parser(subparsers):
    """Add argument parser for extracting trained params from a model
    """
    argparser_extractparams = subparsers.add_parser(
        "extractparams",
        help="extract_params")

    # group for input files
    group_input = argparser_extractparams.add_argument_group(
        "Input files and folders")
    #add_hdf5_data_options(group_input)
    
    # group for parameters
    group_params = argparser_extractparams.add_argument_group(
        "Parameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    
    # group for output files
    group_output = argparser_extractparams.add_argument_group(
        "Output files and folders")
    add_output_options(group_output, out_dir='params')
    
    return


def track_runs(args):
    """track command and github commit
    """
    # keeps track of restores (or different commands) in folder
    num_restores = len(glob.glob('{0}/{1}.command'.format(args.out_dir, args.subcommand_name)))
    logging_file = '{0}/{1}.command_{2}.log'.format(args.out_dir, args.subcommand_name, num_restores)
    
    # track github commit
    git_repo_path = os.path.dirname(os.path.realpath(__file__))
    os.system('echo "commit:" > {0}'.format(logging_file))
    os.system('git --git-dir={0}/.git rev-parse HEAD >> {1}'.format(
        git_repo_path.split("/bin")[0], logging_file))
    os.system('echo "" >> {0}'.format(logging_file))
    
    # write out the command
    with open(logging_file, 'a') as f:
        f.write(' '.join(sys.argv)+'\n\n')
    
    return logging_file


def _setup_logs(args):
    """set up logging
    """
    logging_file = track_runs(args)
    reload(logging)
    logging.basicConfig(
        filename=logging_file,
        level=logging.INFO,
        format='%(message)s')
    logging.getLogger().addHandler(logging.StreamHandler())
    for arg in sorted(vars(args)):
        logging.info("{}: {}".format(arg, getattr(args, arg)))
    logging.info("")

    return


def _is_integer(val):
    """helper function for parsing
    """
    try:
        int(val)
        return True
    except ValueError:
        return False

    
def _parse_files(dataset_key_strings):
    """given an arg string, parse out into a dict
    assumes a format of: key=file1,file2,..::param1=val,param2=val,...
    """
    if dataset_key_strings is None:
        return {}

    key_dict = {}
    for dataset_key_string in dataset_key_strings:
        # first split on "::" then on "="
        key_and_params = dataset_key_string.split("::")
        
        # set up key and key items (filenames or indices)
        key_string = key_and_params[0]
        key_and_items = key_string.split("=")
        key = key_and_items[0]
        if len(key_and_items) == 1:
            key_items = []
        else:
            item_string = key_and_items[1]
            key_items = [
                int(val) if _is_integer(val) else val
                for val in item_string.split(",")]
            
        # set up params
        if len(key_and_params) == 1:
            params = {}
        else:
            param_string = key_and_params[1]
            params = dict(
                [param.split("=")
                 for param in param_string.split(",")])
            
        # combine
        key_dict[key] = (key_items, params)
    
    return key_dict


def _get_absolute_label_indices(model_info, key_dict, list_of_lists=False):
    """given a key dict and model, get the absolute indices
    """
    absolute_task_indices = []
    absolute_task_indices_subsetted = []
    
    start_idx = 0
    for key in model_info["label_keys"]:
        # track how many indices in this key
        with h5py.File(model_info["train_files"][0], "r") as hf:
            num_tasks = hf[key].shape[1]
        if key in key_dict.keys():
            # and create absolute indices and append
            if len(key_dict[key][0]) > 0:
                key_indices = [start_idx + idx for idx in key_dict[key][0]]
            else:
                key_indices = [start_idx + idx for idx in xrange(num_tasks)]
            absolute_task_indices += key_indices
            absolute_task_indices_subsetted.append(key_indices)

        # and then adjust start idx
        start_idx += num_tasks

    if list_of_lists:
        return absolute_task_indices_subsetted
    else:
        return absolute_task_indices


def _setup_annotations(args):
    """load annotation files
    """
    # annotations
    with open(args.annotations, "r") as fp:
        annotations = json.load(fp)
    args.annotations = annotations
    
    return


def _setup_preprocess(args):
    """load preprocess files
    """
    args.labels = _parse_files(args.labels)
    args.signals = _parse_files(args.signals)

    return None


def _setup_model(args):
    """for specific inputs, if they exist, read in
    from json
    """
    # model info
    if args.model_info is not None:
        with open(args.model_info, "r") as fp:
            model_info = json.load(fp)
        args.model_info = model_info
    
    return


def main():
    """Main function for running TRoNN functions
    """
    # parse args and set up
    args = parse_args()
    args.out_dir = os.path.abspath(args.out_dir)
    os.system("mkdir -p {}".format(args.out_dir))
    _setup_logs(args)

    # get subcommand run function and run
    subcommand = args.subcommand_name

    if subcommand == "preprocess":
        _setup_annotations(args)
        _setup_preprocess(args)
        from tronn.preprocess_cmd import run
        run(args)
    elif subcommand == "preprocess_variants":
        _setup_annotations(args)
        _setup_preprocess(args)
        from tronn.preprocess_variants_cmd import run
        run(args)
    elif subcommand == 'train':
        add_model_params(args)
        print 'model args: %s' % args.model
        from tronn.train_cmd import run
        run(args)
    elif subcommand == "evaluate":
        _setup_model(args)
        from tronn.evaluate_cmd import run
        run(args)
    elif subcommand == "predict":
        _setup_model(args)
        from tronn.predict_cmd import run
        run(args)
    elif subcommand == "scanmotifs":
        _setup_model(args)
        _setup_interpretation_options(args)
        _setup_visualization_options(args)
        from tronn.scanmotifs_cmd import run
        run(args)
    elif subcommand == "dmim":
        _setup_model(args)
        _setup_interpretation_options(args)
        _setup_visualization_options(args)
        from tronn.dmim_cmd import run
        run(args)
    elif subcommand == "dream":
        _setup_model(args)
        _setup_interpretation_options(args)
        from tronn.dream_cmd import run
        run(args)
    elif subcommand == "analyzevariants":
        _setup_model(args)
        from tronn.analyzevariants_cmd import run
        run(args)
    elif subcommand == "baseline":
        from tronn.run_tensorforest import run
        run(args)
    elif subcommand == "extractparams":
        from tronn.util.extract_params import run
        run(args)
    # add new commands here
        
    return None


if __name__ == '__main__':
    main()
