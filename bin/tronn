#!/usr/bin/env python

"""Description: TRONN main executable
"""

import os
import sys
import json
import glob
import logging
import argparse
import pkg_resources

from tronn.util.pwms import MotifSetManager # TODO move this?
from tronn.util.scripts import setup_run_logs
from tronn.util.scripts import parse_multi_target_selection_strings

def parse_args():
    """Prepare argument parser. Main subcommands are set up here first
    """
    parser = argparse.ArgumentParser(
        description='TRONN: Transcriptional Regulation Optimized Neural Nets')
    subparsers = parser.add_subparsers(dest='subcommand_name')

    # command for preprocessing data
    add_preprocess_parser(subparsers)
    
    # command for training
    add_train_parser(subparsers)

    # command for evaluation
    add_evaluate_parser(subparsers)

    # command for prediction
    add_predict_parser(subparsers)
    
    # command for motif scan
    add_scanmotifs_parser(subparsers)

    # command for mutate motifs
    add_mutatemotifs_parser(subparsers)

    # command for dmim scan
    add_dmim_parser(subparsers)
    
    # command for network analysis
    add_buildgrammars_parser(subparsers)

    # command for synergy scan
    add_synergy_parser(subparsers)

    # command for grammar characterization
    add_simulategrammar_parser(subparsers)
    
    # command for activation maximization
    add_dream_parser(subparsers)

    # command for rational design with sig motifs
    # add_design_parser(subparsers)
    
    # TODO command for variant predictions
    add_analyzevariants_parser(subparsers)
    
    # baseline model
    add_baseline_parser(subparsers)

    # visualize importance scores parser
    add_buildtracks_parser(subparsers)
    
    # command for extracting model variables
    add_export_model_parser(subparsers)
    
    # parse args
    args = parser.parse_args()

    return args


def _add_output_args(parser, out_dir="./"):
    """Add output directory and prefix args to parser
    """
    parser.add_argument(
        "-o", "--out_dir", dest="out_dir", type=str,
        default=out_dir,
        help = "Output directory (default: current)")
    parser.add_argument(
        "--scratch_dir",
        help = "scratch directory (default: current) - scratch space for faster I/O")
    parser.add_argument(
        "--tmp_dir",
        help = "tmp directory (default: $out_dir/tmp) - where to put data/results that will be deleted after the run")
    parser.add_argument(
        "--copy_data_to_tmp", action="store_true",
        help="if needed (faster disk, reduce network time) copy data to tmp_dir")
    parser.add_argument(
        '--prefix', required=True,
        help='prefix to attach onto file names')

    return None


def _setup_annotations(args):
    """load annotation files from json and replace the json
    file with the dictionary
    """
    with open(args.annotations, "r") as fp:
        annotations = json.load(fp)
    args.annotations = annotations
    
    return None


def _parse_files(dataset_key_strings):
    """given an arg string list, parse out into a dict
    assumes a format of: key=file1,file2,..::param1=val,param2=val,...
    """
    if dataset_key_strings is None:
        return {}

    key_dict = {}
    for dataset_key_string in dataset_key_strings:
        # first split on "::" then on "="
        key_and_params = dataset_key_string.split("::")
        
        # set up key and key items (filenames or indices)
        key_string = key_and_params[0]
        key_and_items = key_string.split("=")
        key = key_and_items[0]
        if len(key_and_items) == 1:
            key_items = []
        else:
            item_string = key_and_items[1]
            key_items = [
                int(val) if _is_integer(val) else val
                for val in item_string.split(",")]
            
        # set up params
        if len(key_and_params) == 1:
            params = {}
        else:
            param_string = key_and_params[1]
            params = dict(
                [param.split("=")
                 for param in param_string.split(",")])
            
        # combine
        key_dict[key] = (key_items, params)
    
    return key_dict


def _setup_preprocess(args):
    """load preprocess files
    """
    args.labels = _parse_files(args.labels)
    args.signals = _parse_files(args.signals)

    return None


def add_preprocess_parser(subparsers):
    """Add data generation function argument parser
    """
    argparser_preprocess = subparsers.add_parser(
        "preprocess",
        help="Preprocess data into TRONN formats")

    # group for input files
    group_input = argparser_preprocess.add_argument_group(
        "Input files and folders")
    group_input.add_argument(
        "--annotations", required=True,
        help="json file of annotation files")
    group_input.add_argument(
        "--labels", nargs='+', required=True,
        help='list of key=files_separated_by_commas::params, files are in BED/narrowPeak format')
    group_input.add_argument(
        "--signals", nargs='+',
        help='list of key=files_separated_by_comma::params, files are in bigwig format')
    group_input.add_argument(
        "--master_label_keys", nargs="+", default=[],
        help="subset of label files to use to make master regions file")
    group_input.add_argument(
        "--master_bed_file", default=None,
        help="master regions file for all examples")

    # group for options
    group_opts = argparser_preprocess.add_argument_group(
        "Data generation options")
    group_opts.add_argument(
        "--rc", action="store_true",
        help='Reverse complement')
    group_opts.add_argument(
        "--genomewide", action="store_true",
        help='build examples across whole genome')
    group_opts.add_argument(
        "--parallel", default=12, type=int,
        help='Number of parallel threads to use')
    
    # group for output files
    group_output = argparser_preprocess.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="./datasets/dataset")

    return


def _add_dataset_args(parser):
    """add dataset args
    """
    parser.add_argument(
        "--data_format", default="hdf5",
        help="dataset format (hdf5, bed, pwm_sims)")
    parser.add_argument(
        "--data_dir",
        help="dataset directory")
    parser.add_argument(
        "--data_files", nargs="+", default=[],
        help="dataset files")
    parser.add_argument(
        "--dataset_json",
        help="use a json as input instead. other data args will supersede this json if requested")
    parser.add_argument(
        "--fasta",
        help="fasta file")
    parser.add_argument(
        "--fifo", action="store_true",
        help="don't shuffle data")
    parser.add_argument(
        "--targets", nargs="+", default=[],
        help="H5 INPUT ONLY: which datasets to load as targets (ordered). Format: whitespace separated list of data_key=indices_list_with_commas::other_params")
    parser.add_argument(
        "--target_indices", nargs="+", default=[], type=int,
        help="H5 INPUT ONLY: after collection of (ordered) targets, which of those to keep. Format: whitespace separated list of data_key=indices_list_with_commas::other_params")
    parser.add_argument(
        "--filter_targets", nargs="+", default=[],
        help="H5 INPUT ONLY: targets to filter on. Format: whitespace separated list of data_key=indices_list_with_commas::other_params")
    parser.add_argument( 
        "--processed_inputs", action="store_true",
        help="H5 INPUT ONLY: inputs are already preprocessed through some inference stack")
    parser.add_argument(
        "--dataset_examples", nargs="+", default=[],
        help="H5 INPUT ONLY: option to throw in specific examples to run")
    
    # add input args if using BED file
    _add_dataset_bed_args(parser)

    return None


def _add_dataset_bed_args(parser):
    """add args specific to BED file input
    """
    parser.add_argument(
        "--bin_width", type=int, default=200,
        help="BED INPUT ONLY: center active region length")
    parser.add_argument(
        "--stride", type=int, default=50,
        help="BED INPUT ONLY: stride (how many bp to skip when going to next interval)")
    parser.add_argument(
        "--final_length", type=int, default=1000,
        help="BED INPUT ONLY: final feature input length")
    parser.add_argument(
        "--num_flanks", type=int, default=3,
        help="BED INPUT ONLY: how many flanks to process on either side")
    
    return None


def _setup_dataset_args(args):
    """set up dataset options
    """
    # first load dataset json if it is given
    if args.dataset_json is not None:
        with open(args.dataset_json, "r") as fp:
            args.dataset_json = json.load(fp)
    else:
        args.dataset_json = {}

    # load data dir from json if not given (args.data_dir supersedes json)
    if args.data_dir is None:
        args.data_dir = args.dataset_json.get("data_dir")
        
    # load data files from json. setting args.data_files supersedes json.
    if len(args.data_files) == 0:
        args.data_files = args.dataset_json.get("data_files", [])
        
    # load targets from json. setting args.targets supersedes json
    if len(args.targets) == 0:
        args.targets = args.dataset_json.get("targets", [])
    else:
        args.targets = parse_multi_target_selection_strings(
            args.targets)

    # load target indices from json. setting args.target_indices supersedes json
    if len(args.target_indices) == 0:
        args.target_indices = args.dataset_json.get("target_indices", [])
    else:
        args.target_indices = parse_multi_target_selection_strings(
            args.target_indices)
    
    # load filter targets from json. setting args.filter_targets supersedes json
    if len(args.filter_targets) == 0:
        args.filter_targets = args.dataset_json.get("filter_targets", [])
    else:
        args.filter_targets = parse_multi_target_selection_strings(
            args.filter_targets)

    # if fasta is not given, check dataset json
    if args.fasta is None:
        args.fasta = args.dataset_json.get("fasta")
    
    return None


def _add_dataset_pwmsims_args(parser):
    """dataset args specifically for simulations with PWMs
    """
    parser.add_argument(
        "--dataset_pwm_file",
        help="PWMSIMS INPUT ONLY: pwm_file")
    parser.add_argument(
        "--sample_range", default=[420, 580], nargs="+", # 420 to 580
        help="PWMSIMS INPUT ONLY: range for embedding in sequence")
    parser.add_argument(
        "--grammar_range", default=[0, 70], nargs="+", # 0 to 120
        help="PWMSIMS INPUT ONLY: range of distances to consider")
    parser.add_argument(
        "--pwm_stride", default=1, type=int,
        help="PWMSIMS INPUT ONLY: bp resolution of tests")
    parser.add_argument(
        "--min_spacing", default=5, type=int,
        help="PWMSIMS INPUT ONLY: bp resolution of tests")
    parser.add_argument(
        "--num_samples", default=30, type=int,
        help="PWMSIMS INPUT ONLY: number of samples for each grammar setup")
    parser.add_argument(
        "--gc_range", default=[0.20, 0.80], nargs="+",
        help="PWMSIMS INPUT ONLY: GC range for random background sequence")
    parser.add_argument(
        "--background_regions",
        help="PWMSIMS INPUT ONLY: bed file from which to select background sequence content")
    parser.add_argument(
        "--embedded_only", action="store_true",
        help="PWMSIMS INPUT ONLY: don't output the original background sequence")

    # args if doing single pwm analyses (homotypic clustering)
    parser.add_argument(
        "--single_pwm", action="store_true",
        help="PWMSIMS INPUT ONLY: pwm sims for single pwm analyses. REQURES data_files is a text file with a single pwm name")
    parser.add_argument(
        "--count_range", default=[1, 6], nargs="+",
        help="PWMSIMS INPUT ONLY: number of PWMs to embed")
    
    return None


def _setup_dataset_pwmsims_args(args):
    """set up simulate grammar args
    """
    # make sure data files is a list of length 1
    assert len(args.data_files) == 1
    
    # load pwm file
    if args.dataset_pwm_file is not None:
        args.dataset_pwm_list = MotifSetManager.read_pwm_file(args.dataset_pwm_file)
        args.dataset_pwm_names = [pwm.name for pwm in args.dataset_pwm_list]
        args.dataset_pwm_dict = MotifSetManager.read_pwm_file(args.dataset_pwm_file, as_dict=True)
    else:
        args.dataset_pwm_list = None
        args.dataset_pwm_names = None
        args.dataset_pwm_dict = None

    # make sure dtypes are right
    args.sample_range = [int(val) for val in args.sample_range]
    args.grammar_range = [int(val) for val in args.grammar_range]
    args.gc_range = [float(val) for val in args.gc_range]

    assert len(args.sample_range) == 2
    assert len(args.grammar_range) == 2
    assert len(args.gc_range) == 2
    
    # only used for single pwm analyses
    args.count_range = [int(val) for val in args.count_range]
    assert len(args.count_range) == 2
    
    return None


def _add_model_args(parser):
    """add model args
    """
    parser.add_argument(
        "--model_framework", default="tensorflow",
        help="deep learning framework (tensorflow(tronn), keras, pytorch)")
    parser.add_argument(
        "--model", nargs="+",
        help="net function")
    parser.add_argument(
        "--model_json",
        help="use json as input instead")
    parser.add_argument(
        "--model_dir",
        help="model directory (if you want to update path)")
    parser.add_argument(
        "--model_checkpoint",
        help="model checkpoint (if you want to update checkpoint)")
    parser.add_argument(
        "--logit_indices", nargs="+", default=[], type=int,
        help="which logits from net to load")
    parser.add_argument(
        "--num_gpus", default=1, type=int,
        help="how many GPUs are available")

    return None


def _setup_model_args(args):
    """set up model options that came in through cmd line
    assumes a format of: model_name param=val,param=val,...

    Returns:
      converts args.model to dict of all model inputs
    """
    # first set up everything through args.model
    if args.model is not None:
        model = {
            "name": args.model[0],
            "params": {}}
        for model_arg in args.model[1:]:
            if '=' in model_arg:
                # params
                key, value = model_arg.split('=', 1)
                # check if param is a list
                if "," in value:
                    model["params"][key] = value.split(",")
                else:
                    model["params"][key] = eval(value)
            else:
                # action store true
                model["params"][model_arg] = True
        args.model = model
    else:
        with open(args.model_json, "r") as fp:
            args.model = json.load(fp)

    # update checkpoint if needed
    if args.model_checkpoint is not None:
        args.model["checkpoint"] = args.model_checkpoint

    # and update for model json dir
    if (args.model_json is not None) and (args.model_framework != "keras"):
        args.model["checkpoint"] = "{}/train/{}".format(
            os.path.dirname(args.model_json),
            os.path.basename(args.model["checkpoint"]))
        
    # update model dir (and checkpoint location) if needed
    if args.model_dir is not None:
        args.model["model_dir"] = args.model_dir
        if args.model.get("checkpoint") is not None:
            args.model["checkpoint"] = "{}/{}".format(
                args.model_dir,
                os.path.basename(args.model["checkpoint"]))

    # if model dir is still none, use out_dir
    if args.model.get("model_dir") is None:
        args.model["model_dir"] = args.out_dir
    
    return None


def _setup_warm_start_args(args):
    """set up warm start args
    """
    # first, extract model json if defined
    if args.transfer_model_json is not None:
        with open(args.transfer_model_json, "r") as fp:
            args.transfer_model = json.load(fp)
    else:
        args.transfer_model = {}
            
    # only update transfer checkpoint if it was None
    if args.transfer_checkpoint is None:
        args.transfer_checkpoint = args.transfer_model.get("checkpoint")

    # add skip vars to warm start params
    args.transfer_params = {"skip": args.transfer_skip_vars}
    
    return None


def add_train_parser(subparsers):
    """Add argument parser for training
    """
    cmd_argparser = subparsers.add_parser(
        "train",
        help="Train a NN model")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    
    # group for cross validation params
    group_cv = cmd_argparser.add_argument_group(
        "Cross validation params")
    group_cv.add_argument(
        "--full_train", action="store_true",
        help="ignore early stopping, train for full num epochs")
    group_cv.add_argument(
        "--kfolds", default=10, type=int,
        help="number of folds to split the data")
    group_cv.add_argument(
        "--valid_folds", nargs="+", default=[0], type=int,
        help="which fold to use as validation")
    group_cv.add_argument(
        "--test_folds", nargs="+", default=[1], type=int,
        help="which fold to use as test")

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)

    # group for warm starting
    group_transfer = cmd_argparser.add_argument_group(
        "Transfer model arguments")
    group_transfer.add_argument(
        "--transfer_model_json", default=None,
        help="transfer model json. must have a checkpoint")
    group_transfer.add_argument(
        "--transfer_checkpoint", default=None,
        help="transfer model checkpoint, if defined will supersede model json")
    group_transfer.add_argument(
        "--transfer_skip_vars", nargs="+", default=[],
        help="variable names to skip (note searches these as substrings of var names)")
    group_transfer.add_argument(
        "--use_transfer_splits", action="store_true",
        help="set up train/valid/test splits using transfer model info")
    
    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "Training arguments")
    group_params.add_argument(
        '--epochs', default=30, type=int,
        help='number of epochs')
    group_params.add_argument(
        '--batch_size', default=256, type=int,
        help='batch size')
    group_params.add_argument(
        '--early_stopping_metric', default='mean_auprc', type=str,
        help='metric to use for early stopping')
    group_params.add_argument(
        '--patience', default=3, type=int,
        help='metric to use for early stopping')
    group_params.add_argument(
        "--regression", action="store_true",
        help="run regression instead of classification")
    group_params.add_argument(
        "--distributed", action="store_true",
        help="run on multi gpus")
    group_params.add_argument(
        '--finetune_targets', nargs="+", default=[],
        help="tasks to finetune")

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='model')
    group_output.add_argument(
        "--get_dataset_metrics", action="store_true",
        help="calculate dataset metrics if needed")

    return


def add_evaluate_parser(subparsers):
    """Add argument parser for test-time evaluation
    """
    cmd_argparser = subparsers.add_parser(
        "evaluate",
        help="Evaluate model")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    
    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "Parameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
    	"--regression", action="store_true",
    	help="evaluate regression")
    group_params.add_argument(
    	"--num_evals", default=64000, type=int,
    	help="Number of steps to run for prediction")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='evaluation')
    
    return


def _add_outlayer_args(parser):
    """adjustments at the output layer args
    """
    parser.add_argument(
        "--prediction_sample", default=None,
        help="file of predictions for quantile norm")
    parser.add_argument(
        "--h5_saver_batch_size", default=2048, type=int,
        help="chunking before saving to h5 (improve I/O)")
    parser.add_argument(
        "--sample_size", default=10000000, type=int,
        help="total regions (post filtering) to return")
    
    return


def add_predict_parser(subparsers):
    """Add argument parser for predicting with trained models
    """
    cmd_argparser = subparsers.add_parser(
        "predict",
        help="Predict with trained model")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    _add_dataset_pwmsims_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)

    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "Parameters")
    group_params.add_argument(
        '--batch_size', default=128, type=int,
        help='batch size')
    group_params.add_argument(
    	"--regression", action="store_true",
    	help="evaluate regression")
    group_params.add_argument(
    	"--num_evals", default=10000000, type=int,
    	help="Number of steps to run for prediction")
    _add_outlayer_args(group_params)
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='prediction')
    
    return


def _add_backprop_args(parser):
    """backprop interpretation args
    """
    parser.add_argument(
        "--inference_targets", nargs="+", type=int,
        help="tasks to extract importance scores - must be global indices") 
    parser.add_argument(
        "--backprop", default="input_x_grad",
        help="what method to use for interpretation")
    
    return


def _add_pwm_conv_args(parser):
    """args pertaining specifically to pwm conv layer
    """
    parser.add_argument(
        "--pwm_file", default=None,
        help="pwm file")
    parser.add_argument(
        "--add_rc_pwms", action="store_true",
        help="also load the pwms in the reverse complement orientation")
    
    return


def _setup_pwm_conv_args(args):
    """args pertaining specifically to pwm conv layer
    """
    # read in pwm file
    if args.pwm_file is not None:
        args.pwm_list = MotifSetManager.read_pwm_file(args.pwm_file)
        args.pwm_names = [pwm.name for pwm in args.pwm_list]
        args.pwm_dict = MotifSetManager.read_pwm_file(args.pwm_file, as_dict=True)
    else:
        args.pwm_list = None
        args.pwm_names = None
        args.pwm_dict = None
    
    return


def _add_interpretation_args(parser, inference_fn="sequence_to_motif_scores"):
    """set up trained model info
    """
    parser.add_argument(
        "--infer_json",
        help="inference json file to continue from last run")
    _add_backprop_args(parser)
    parser.add_argument(
        "--inference_fn", default=inference_fn,
        help="inference stack")
    parser.add_argument(
        "--left_clip", default=420, type=int,
        help="how much to clip off the left sequence flank")
    parser.add_argument(
        "--right_clip", default=580, type=int,
        help="how much to clip off the right sequence flank")
    _add_outlayer_args(parser)
    parser.add_argument(
        "--use_filtering", action="store_true",
        help="filter out examples with no importance scores and/or no motif hits") # todo split out motif/impt filters
    parser.add_argument(
        "--debug", action="store_true",
        help="visualize outputs to help debug. NOTE: use small sample size!")

    # pwm conv layer
    _add_pwm_conv_args(parser)
    
    return


def _setup_interpretation_args(args):
    """given parsed args, adjust inputs as needed
    matches _add_interpretation_options
    """
    # load infer json if it exists
    if args.infer_json is not None:
        with open(args.infer_json, "r") as fp:
            args.infer_json = json.load(fp)
    else:
        args.infer_json = {}

    # load out of infer json if not overridden in cmd line
    if args.model_json is None:
        args.model_json = args.infer_json.get("model_json")
    if args.inference_targets is None:
        args.inference_targets = args.infer_json.get("inference_targets")
    if args.pwm_file is None:
        args.pwm_file = args.infer_json.get("pwm_file")
            
    # pwm file
    _setup_pwm_conv_args(args)
    
    # set up inference dict
    args.inference_params = {
        "cmd_name": args.subcommand_name,
        "inference_mode": True, # for estimator framework
        "model_reuse": True if args.model_framework == "tensorflow" else False,
        "inference_fn_name": args.inference_fn,
        "importance_task_indices": args.inference_targets,
        "backprop": args.backprop,
        "left_clip": args.left_clip,
        "right_clip": args.right_clip,
        "use_filtering": args.use_filtering,
        "prediction_sample": args.prediction_sample,
        "pwms": args.pwm_list,
        "add_rc_pwms": True if args.add_rc_pwms else False}

    return


def _add_mutate_args(parser):
    """add args for mutational analyses
    """
    parser.add_argument(
        "--mutate_type", default="shuffle",
        help="what kind of mutation to do (point or shuffle)")
    
    return

def _setup_mutate_args(args):
    """add in mutate args as needed
    """
    args.inference_params.update({"mutate_type": args.mutate_type})
    
    return


def _add_sig_pwms_args(parser):
    """add in args to use sig pwm vectors
    """
    parser.add_argument(
        "--sig_pwms_file",
        help="if this arg is used, pvals will be used from here and not from scan file")
    parser.add_argument(
        "--sig_pwms_key", default="pvals",
        help="which dataset group has the sig pwm vectors")

    return


def _add_visualization_args(parser):
    """set up visualization after interpretation options
    """
    parser.add_argument(
        "--visualize_R", nargs="+", default=[],
        help="visualize with R scripts")
    parser.add_argument(
        "--visualize_multikey_R", nargs="+", default=[],
        help="visualize with R scripts")
    
    return


def _setup_visualization_args(args):
    """given parsed args, adjust visualization inputs as needed
    matches _add_visualization_options
    """
    if args.visualize_R is not None:
        args.visualize_R = _parse_files(args.visualize_R)
    if args.visualize_multikey_R is not None:
        args.visualize_multikey_R = _parse_files(args.visualize_multikey_R)

    return


def add_scanmotifs_parser(subparsers):
    """Scan motifs
    """
    cmd_argparser = subparsers.add_parser(
        "scanmotifs",
        help="Scan sequence with motifs from a PWM file")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=48, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_motif_scores") # adjust for defaults
    _add_visualization_args(group_interpret)
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="motifs")
    group_output.add_argument(
        "--lite", action="store_true",
        help="store only most essential tensors (lite output)")

    return


def add_buildgrammars_parser(subparsers):
    """build grammars from motif scores or mutational results
    """
    cmd_argparser = subparsers.add_parser(
        "buildgrammars",
        help="build grammars from scanning results (motif or mutational)")

    # group for type
    group_analysis = cmd_argparser.add_argument_group(
        "Grammar build type")
    group_analysis.add_argument(
        "--scan_type", default="dmim",
        help="type of results being used (hits, impt, mut)")
    group_analysis.add_argument(
        "--min_positive_tasks", default=2, type=int,
        help="minimum cell states (tasks) in which effect exists to generate edge")
    group_analysis.add_argument(
        "--subgraph_max_k", default=2, type=int,
        help="when searching graph, max nodes to include in a subgraph")
    group_analysis.add_argument(
        "--min_support_fract", default=0.10, type=float,
        help="minimum region fract that is required for a grammar (overruled by min support if min support is larger)")
    group_analysis.add_argument(
        "--min_support", default=200, type=int,
        help="min number of examples needed to support a grammar")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    group_dataset.add_argument(
        "--scan_file", required=True,
        help="h5 file of scan results")
    group_dataset.add_argument(
        "--foreground_targets", required=True,
        help="key to pvals")
    group_dataset.add_argument(
        "--rc_pwms_present", action="store_true",
        help="whether RC pwms are in dataset")
    group_dataset.add_argument(
        "--keep_grammars", nargs="+", default=[],
        help="grammars to keep IF they exist. use pwm names separated by commas")
    group_dataset.add_argument(
        "--ignore_pwms", nargs="+", default=[],
        help="motifs to ignore in grammar building")
    group_dataset.add_argument(
        "--aux_data_keys", nargs="+", default=[],
        help="extra dataset summaries to add")
    _add_sig_pwms_args(group_dataset)
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="grammars")
    
    return


def add_mutatemotifs_parser(subparsers):
    """scan grammars from a grammar file
    """
    cmd_argparser = subparsers.add_parser(
        "mutatemotifs",
        help="Mutate motifs and predict results")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    _add_sig_pwms_args(group_dataset)
    group_dataset.add_argument(
        "--batch_size", default=4, type=int,
        help="batch size")
    
    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
        
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--infer_json",
        help="load in infer json from scanmotifs (get pwm file)")
    _add_mutate_args(
        group_interpret)
    _add_outlayer_args(group_interpret)

    # group for cmd specific functions
    group_other = cmd_argparser.add_argument_group(
        "Misc params")
    group_other.add_argument(
        "--foreground_targets", nargs="+",
        help="must match label in sig pwms file!")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="mutatemotifs")
    
    return


def add_dmim_parser(subparsers):
    """scan grammars from a grammar file
    """
    cmd_argparser = subparsers.add_parser(
        "dmim",
        help="score sequences for grammars")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    _add_sig_pwms_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
        
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=8, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_dmim")
    _add_mutate_args(
        group_interpret)
    _add_visualization_args(group_interpret)

    # group for cmd specific functions
    group_other = cmd_argparser.add_argument_group(
        "Misc params")
    group_other.add_argument(
        "--foreground_targets", nargs="+",
        help="which targets to use as foreground for calling motif enrichment")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="dmim")
    
    return


def add_synergy_parser(subparsers):
    """scan grammars from a grammar file
    """
    cmd_argparser = subparsers.add_parser(
        "synergy",
        help="score sequences for grammars")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    _add_dataset_pwmsims_args(group_dataset)
    
    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
        
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=8, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_synergy")
    _add_mutate_args(
        group_interpret)
    _add_visualization_args(group_interpret)

    # group for cmd specific functions
    group_other = cmd_argparser.add_argument_group(
        "Misc params")
    group_other.add_argument(
        "--grammar_file", help="grammar (gml) file")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="dmim")
    
    return


def add_simulategrammar_parser(subparsers):
    """build synthetic sequences to simulate grammar
    """
    cmd_argparser = subparsers.add_parser(
        "simulategrammar",
        help="Build synthetic sequences with grammar embedded")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    _add_dataset_pwmsims_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    
    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=8, type=int,
        help="internal batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_motif_scores")
        
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="simulategrammar")
    
    return
        

def add_dream_parser(subparsers):
    """activation maximization
    """
    cmd_argparser = subparsers.add_parser(
        "dream",
        help="use activation maximization to generate novel synthetic sequences")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset) # use BED input? or array input

    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    _add_interpretation_args(group_interpret)

    # group for parameters
    group_params = cmd_argparser.add_argument_group(
        "dream hyperparameters")
    group_params.add_argument(
        "--max_iter", default=20, type=int,
        help="number of iterations for dreaming")
    group_params.add_argument(
        "--edit_dist", default=1000, type=int,
        help="edit distance from start position allowed")

    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="dream")
    group_output.add_argument(
        "--plot", action="store_true",
        help="show development of sequence across time")

    return 


def add_analyzevariants_parser(subparsers):
    """analyze variants
    """
    cmd_argparser = subparsers.add_parser(
        "analyzevariants",
        help="Analyze variants from a VCF file")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    group_dataset.add_argument(
        "--data_format", default="vcf",
        help="format for dataloader")
    group_dataset.add_argument(
        "--vcf_file",
        help="VCF formatted input file of variants")
    group_dataset.add_argument(
        "--fasta",
        help="fasta file")

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)

    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=32, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="variants_to_scores")
    _add_visualization_args(group_interpret)
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="motifs")
    
    return


def add_baseline_parser(subparsers):
    """Add train baseline function argument parser
    """
    argparser_baseline = subparsers.add_parser("baseline", help="Run baseline model")

    # group for input files
    group_input = argparser_baseline.add_argument_group("Input files and folders")
    group_input.add_argument('--data_dir', help='Data directory of kmer hdf5 files')
    group_input.add_argument(
        "--cvfold", default=0, type=int,
        help="which files to train on [0, 1, 2]")
    group_input.add_argument(
        "--num_classes", default=2, type=int,
        help="how many classes")
    group_input.add_argument(
        "--kmers", action="store_true",
        help="featurize as kmers")
    group_input.add_argument(
        "--kmer_len", default=6, type=int,
        help="what kmer len to featurize with")
    group_input.add_argument(
        "--motifs", action="store_true",
        help="featurize as kmers")
    group_input.add_argument(
        "--pwm_file",
        help="motif_file")
    group_input.add_argument(
        "--tasks", nargs="+", default=[], type=int,
        help="tasks to train on (default is all)")

    # group for parameters
    group_params = argparser_baseline.add_argument_group("Training hyperparameters")
    group_params.add_argument('--batch_size', default=128, type=int, help='batch size')
    group_params.add_argument('--num_trees', default=100, type=int, help='how many trees')
    group_params.add_argument('--max_nodes', default=10000, type=int, help='max nodes per tree')
    group_params.add_argument('--num_evals', default=1000, type=int, help='num evaluation batches')

    # group for output files
    group_output = argparser_baseline.add_argument_group("Output files and folders")
    _add_output_args(group_output, out_dir='baseline')

    return


def add_buildtracks_parser(subparsers):
    """plot a region, either command line region ID or BED file of regions
    """
    cmd_argparser = subparsers.add_parser(
        "buildtracks",
        help="plot importance scores for a region")
    
    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)
    group_dataset.add_argument(
        "--chromsizes",
        help="chromsizes file")
    
    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)

    # group for interpretation params
    group_interpret = cmd_argparser.add_argument_group(
        "Interpretation params")
    group_interpret.add_argument(
        "--batch_size", default=32, type=int,
        help="batch size")
    _add_interpretation_args(
        group_interpret,
        inference_fn="sequence_to_importances")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir="grammars")
    
    return


def add_export_model_parser(subparsers):
    """Add argument parser for extracting trained params from a model
    """
    cmd_argparser = subparsers.add_parser(
        "exportmodel",
        help="extract model variables")

    # group for dataset
    group_dataset = cmd_argparser.add_argument_group(
        "Dataset arguments")
    _add_dataset_args(group_dataset)

    # group for model
    group_model = cmd_argparser.add_argument_group(
        "Model arguments")
    _add_model_args(group_model)
    group_model.add_argument(
        "--skip", nargs="+", default=[],
        help="substrings in variable names to skip")
    
    # group for output files
    group_output = cmd_argparser.add_argument_group(
        "Output files and folders")
    _add_output_args(group_output, out_dir='export')
    
    return


def _is_integer(val):
    """helper function for parsing
    """
    try:
        int(val)
        return True
    except ValueError:
        return False
    

def main():
    """Main function for running TRoNN functions
    """
    # parse args 
    args = parse_args()

    # set up scratch dir as needed
    if args.scratch_dir is not None:
        os.system("mkdir -p {}".format(args.scratch_dir))
        out_dir_final = str(args.out_dir)
        args.out_dir = args.scratch_dir
    else:
        out_dir_final = args.out_dir

    # set up tmp dir
    if args.tmp_dir is None:
        args.tmp_dir = "{}/tmp".format(args.out_dir)
    os.system("mkdir -p {}".format(args.tmp_dir))

    # if user wants to copy data over to tmp set up args.tmp_data_dir
    if args.copy_data_to_tmp:
        args.tmp_data_dir = args.tmp_dir
    else:
        args.tmp_data_dir = None
        
    # set up logs
    args.out_dir = os.path.abspath(args.out_dir)
    os.system("mkdir -p {}".format(args.out_dir))
    setup_run_logs(args, args.subcommand_name)
    
    # get subcommand run function and run
    subcommand = args.subcommand_name

    if subcommand == "preprocess":
        _setup_annotations(args)
        _setup_preprocess(args)
        from tronn.preprocess_cmd import run
        run(args)
    elif subcommand == 'train':
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_warm_start_args(args)
        from tronn.train_cmd import run
        run(args)
    elif subcommand == "evaluate":
        _setup_dataset_args(args)
        _setup_model_args(args)
        from tronn.evaluate_cmd import run
        run(args)
    elif subcommand == "predict":
        _setup_dataset_args(args)
        _setup_dataset_pwmsims_args(args)
        _setup_model_args(args)
        from tronn.predict_cmd import run
        run(args)
    elif subcommand == "scanmotifs":
        _setup_interpretation_args(args)
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_visualization_args(args)
        from tronn.scanmotifs_cmd import run
        run(args)
    elif subcommand == "mutatemotifs":
        _setup_dataset_args(args)
        _setup_model_args(args)
        from tronn.mutatemotifs_cmd import run
        run(args)
    elif subcommand == "dmim":
        _setup_interpretation_args(args)
        _setup_mutate_args(args)
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_visualization_args(args)
        from tronn.dmim_cmd import run
        run(args)
    elif subcommand == "buildgrammars":
        from tronn.buildgrammars_cmd import run
        run(args)
    elif subcommand == "synergy":
        _setup_interpretation_args(args)
        _setup_mutate_args(args)
        _setup_dataset_args(args)
        _setup_dataset_pwmsims_args(args)
        _setup_model_args(args)
        _setup_visualization_args(args)
        from tronn.synergy_cmd import run
        run(args)
    elif subcommand == "simulategrammar":
        _setup_interpretation_args(args)
        _setup_dataset_pwmsims_args(args)
        _setup_model_args(args)
        # force dataset args (do this in cmd??)
        args.data_format = "pwm_sims"
        args.fifo = True
        from tronn.simulategrammars_cmd import run
        run(args)
    elif subcommand == "dream":
        _setup_dataset_args(args)
        _setup_model_args(args)
        _setup_interpretation_options(args)
        from tronn.dream_cmd import run
        run(args)
    elif subcommand == "analyzevariants":
        _setup_interpretation_args(args)
        _setup_model_args(args)
        from tronn.analyzevariants_cmd import run
        run(args)
    elif subcommand == "baseline":
        _setup_dataset_args(args)
        from tronn.run_tensorforest import run
        run(args)
    elif subcommand == "buildtracks":
        _setup_interpretation_args(args)
        _setup_model_args(args)
        from tronn.buildtracks_cmd import run
        run(args)
    elif subcommand == "exportmodel":
        _setup_dataset_args(args)
        _setup_model_args(args)
        from tronn.export_cmd import run
        run(args)
    else:
        raise ValueError, "command not implemented!"

    # clear the tmp dir
    os.system("rm -r {}".format(args.tmp_dir))
    
    # if used scratch_dir, copy back
    if out_dir_final != args.out_dir:
        copy_cmd = "rsync -avz --progress {}/ {}/".format(args.out_dir, out_dir_final)
        logging.info(copy_cmd)
        os.system(copy_cmd)
    
    return None


if __name__ == '__main__':
    main()
